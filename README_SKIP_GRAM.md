# ğŸ¯ Thuáº­t ToÃ¡n Skip-gram

TÃ i liá»‡u chi tiáº¿t vá» thuáº­t toÃ¡n Skip-gram - ná»n táº£ng cá»§a Word2Vec vÃ  DeepWalk.

## ğŸ“‹ Tá»•ng quan

Skip-gram lÃ  thuáº­t toÃ¡n **máº¡ng neural** Ä‘á»ƒ há»c **word embeddings** (hoáº·c node embeddings trong DeepWalk). Ã tÆ°á»Ÿng cá»‘t lÃµi: **dá»± Ä‘oÃ¡n cÃ¡c tá»« ngá»¯ cáº£nh tá»« tá»« trung tÃ¢m**.

### **KhÃ¡c biá»‡t vá»›i CBOW:**
- **Skip-gram**: Tá»« trung tÃ¢m â†’ Dá»± Ä‘oÃ¡n cÃ¡c tá»« ngá»¯ cáº£nh
- **CBOW**: CÃ¡c tá»« ngá»¯ cáº£nh â†’ Dá»± Ä‘oÃ¡n tá»« trung tÃ¢m

## ğŸ¯ Má»¥c tiÃªu cá»§a Skip-gram

Cho má»™t **tá»« trung tÃ¢m**, tá»‘i Ä‘a hÃ³a xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cÃ¡c **tá»« ngá»¯ cáº£nh** xung quanh nÃ³.

### **HÃ m má»¥c tiÃªu toÃ¡n há»c:**

$$
\text{maximize} \quad \mathcal{L} = \sum_{w \in \text{corpus}} \sum_{c \in C(w)} \log P(c | w)
$$

Trong Ä‘Ã³:
- $w$: tá»« trung tÃ¢m  
- $C(w)$: cÃ¡c tá»« ngá»¯ cáº£nh cá»§a $w$
- $P(c | w)$: xÃ¡c suáº¥t dá»± Ä‘oÃ¡n tá»« ngá»¯ cáº£nh $c$ khi biáº¿t tá»« trung tÃ¢m $w$

## ğŸ“– VÃ­ dá»¥ minh há»a cá»¥ thá»ƒ

### **CÃ¢u Ä‘áº§u vÃ o:**
```
"Há»c sinh Ä‘i há»c táº¡i trÆ°á»ng Ä‘áº¡i há»c"
```

### **Sau khi tÃ¡ch tá»«:**
```
[há»c, sinh, Ä‘i, há»c, táº¡i, trÆ°á»ng, Ä‘áº¡i, há»c]
```

### **KÃ­ch thÆ°á»›c cá»­a sá»• ngá»¯ cáº£nh = 2:**

| Vá»‹ trÃ­ | Tá»« trung tÃ¢m | Cá»­a sá»• ngá»¯ cáº£nh | CÃ¡c tá»« ngá»¯ cáº£nh |
|--------|-------------|----------------|-----------------|
| 0 | há»c | [-, -, **há»c**, sinh, Ä‘i] | [sinh, Ä‘i] |
| 1 | sinh | [-, **há»c**, **sinh**, Ä‘i, há»c] | [há»c, Ä‘i, há»c] |
| 2 | Ä‘i | [**há»c**, **sinh**, **Ä‘i**, há»c, táº¡i] | [há»c, sinh, há»c, táº¡i] |
| 3 | há»c | [**sinh**, **Ä‘i**, **há»c**, táº¡i, trÆ°á»ng] | [sinh, Ä‘i, táº¡i, trÆ°á»ng] |
| 4 | táº¡i | [**Ä‘i**, **há»c**, **táº¡i**, trÆ°á»ng, Ä‘áº¡i] | [Ä‘i, há»c, trÆ°á»ng, Ä‘áº¡i] |

### **VÃ­ dá»¥ huáº¥n luyá»‡n:**
```python
# Tá»« vá»‹ trÃ­ 1 (tá»« trung tÃ¢m = "sinh"):
VÃ­ dá»¥ huáº¥n luyá»‡n:
- Äáº§u vÃ o: "sinh" â†’ Má»¥c tiÃªu: "há»c" 
- Äáº§u vÃ o: "sinh" â†’ Má»¥c tiÃªu: "Ä‘i"
- Äáº§u vÃ o: "sinh" â†’ Má»¥c tiÃªu: "há»c" (láº§n 2)

# Tá»« vá»‹ trÃ­ 2 (tá»« trung tÃ¢m = "Ä‘i"):  
VÃ­ dá»¥ huáº¥n luyá»‡n:
- Äáº§u vÃ o: "Ä‘i" â†’ Má»¥c tiÃªu: "há»c"
- Äáº§u vÃ o: "Ä‘i" â†’ Má»¥c tiÃªu: "sinh" 
- Äáº§u vÃ o: "Ä‘i" â†’ Má»¥c tiÃªu: "há»c" (láº§n 2)
- Äáº§u vÃ o: "Ä‘i" â†’ Má»¥c tiÃªu: "táº¡i"
```

## ğŸ§® Kiáº¿n trÃºc Skip-gram

### **Cáº¥u trÃºc máº¡ng Neural Network:**

```
Lá»›p Ä‘áº§u vÃ o      Lá»›p áº©n        Lá»›p Ä‘áº§u ra
(One-hot)       (Embeddings)    (Softmax)

   há»c              [0.2]           P(há»c|sinh)
   sinh      â†’      [0.5]      â†’    P(Ä‘i|sinh)  
   Ä‘i               [0.1]           P(táº¡i|sinh)
   táº¡i              [0.8]           ...
   ...              [...]
```

### **CÃ´ng thá»©c toÃ¡n há»c:**

**Äáº§u vÃ o**: Vector one-hot $\mathbf{x}_w$ cho tá»« trung tÃ¢m $w$

**Lá»›p áº©n**: Vector embedding
$$
\mathbf{h} = \mathbf{W}^T \mathbf{x}_w = \mathbf{v}_w
$$

**Lá»›p Ä‘áº§u ra**: PhÃ¢n phá»‘i xÃ¡c suáº¥t
$$
P(c | w) = \frac{\exp(\mathbf{u}_c^T \mathbf{v}_w)}{\sum_{j=1}^{|V|} \exp(\mathbf{u}_j^T \mathbf{v}_w)}
$$

Trong Ä‘Ã³:
- $\mathbf{v}_w$: **input embedding** cá»§a tá»« $w$
- $\mathbf{u}_c$: **output embedding** cá»§a tá»« ngá»¯ cáº£nh $c$  
- $|V|$: kÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn (vocabulary size)

## ğŸ”¢ VÃ­ dá»¥ tÃ­nh toÃ¡n cá»¥ thá»ƒ

### **Thiáº¿t láº­p:**
```python
vocabulary = ["há»c", "sinh", "Ä‘i", "táº¡i", "trÆ°á»ng"]
vocab_size = 5
embedding_dim = 3
```

### **BÆ°á»›c 1: Khá»Ÿi táº¡o embeddings**
```python
# Input embeddings (W_in) - ma tráº­n 5x3
W_in = [
    [0.1, 0.2, 0.3],  # há»c
    [0.4, 0.5, 0.6],  # sinh  
    [0.7, 0.8, 0.9],  # Ä‘i
    [0.2, 0.3, 0.4],  # táº¡i
    [0.5, 0.6, 0.7]   # trÆ°á»ng
]

# Output embeddings (W_out) - ma tráº­n 5x3  
W_out = [
    [0.9, 0.8, 0.7],  # há»c
    [0.6, 0.5, 0.4],  # sinh
    [0.3, 0.2, 0.1],  # Ä‘i  
    [0.7, 0.6, 0.5],  # táº¡i
    [0.4, 0.3, 0.2]   # trÆ°á»ng
]
```

### **BÆ°á»›c 2: Lan truyá»n xuÃ´i (Forward pass)**
**VÃ­ dá»¥ huáº¥n luyá»‡n**: Tá»« trung tÃ¢m = "sinh" (chá»‰ sá»‘ 1), Má»¥c tiÃªu = "Ä‘i" (chá»‰ sá»‘ 2)

```python
# Láº¥y input embedding cho "sinh"
v_sinh = W_in[1] = [0.4, 0.5, 0.6]

# TÃ­nh Ä‘iá»ƒm cho táº¥t cáº£ cÃ¡c tá»«
scores = []
for i in range(vocab_size):
    u_i = W_out[i]
    score = dot_product(u_i, v_sinh)
    scores.append(score)

# VÃ­ dá»¥ tÃ­nh toÃ¡n:
# score("há»c") = [0.9, 0.8, 0.7] Â· [0.4, 0.5, 0.6] = 0.36 + 0.40 + 0.42 = 1.18
# score("sinh") = [0.6, 0.5, 0.4] Â· [0.4, 0.5, 0.6] = 0.24 + 0.25 + 0.24 = 0.73
# score("Ä‘i") = [0.3, 0.2, 0.1] Â· [0.4, 0.5, 0.6] = 0.12 + 0.10 + 0.06 = 0.28
```

### **BÆ°á»›c 3: Ãp dá»¥ng Softmax**
```python
scores = [1.18, 0.73, 0.28, 1.02, 0.53]

# Ãp dá»¥ng softmax
exp_scores = [exp(s) for s in scores] = [3.25, 2.07, 1.32, 2.77, 1.70]
sum_exp = sum(exp_scores) = 11.11

# TÃ­nh xÃ¡c suáº¥t
probabilities = [s/sum_exp for s in exp_scores]
P = [0.293, 0.186, 0.119, 0.249, 0.153]

# P(Ä‘i | sinh) = 0.119
```

### **BÆ°á»›c 4: TÃ­nh toÃ¡n hÃ m máº¥t mÃ¡t**
```python
# Cross-entropy loss cho má»¥c tiÃªu "Ä‘i" (chá»‰ sá»‘ 2)
target_prob = P[2] = 0.119
loss = -log(target_prob) = -log(0.119) = 2.13
```

## ğŸ”„ Tá»‘i Æ°u hÃ³a Negative Sampling

### **Váº¥n Ä‘á» vá»›i vanilla softmax:**
TÃ­nh toÃ¡n softmax cho tá»« Ä‘iá»ƒn lá»›n (10K-100K tá»«) ráº¥t tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n!

### **Giáº£i phÃ¡p: Negative Sampling**

Thay vÃ¬ dá»± Ä‘oÃ¡n trÃªn toÃ n bá»™ tá»« Ä‘iá»ƒn, chá»‰ phÃ¢n biá»‡t giá»¯a:
- 1 **vÃ­ dá»¥ dÆ°Æ¡ng** (tá»« ngá»¯ cáº£nh tháº­t)
- $k$ **vÃ­ dá»¥ Ã¢m** (cÃ¡c tá»« ngáº«u nhiÃªn)

### **HÃ m má»¥c tiÃªu:**
$$
\log \sigma(\mathbf{u}_c^T \mathbf{v}_w) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} [\log \sigma(-\mathbf{u}_{w_i}^T \mathbf{v}_w)]
$$

Trong Ä‘Ã³:
- $\sigma(x) = \frac{1}{1 + e^{-x}}$: hÃ m sigmoid
- $P_n(w) \propto U(w)^{3/4}$: phÃ¢n phá»‘i láº¥y máº«u Ã¢m
- $k = 5-20$: sá»‘ lÆ°á»£ng negative samples

### **VÃ­ dá»¥ Negative Sampling:**
```python
# VÃ­ dá»¥ huáº¥n luyá»‡n: "sinh" â†’ "Ä‘i" (dÆ°Æ¡ng)
positive_pair = ("sinh", "Ä‘i")

# CÃ¡c máº«u Ã¢m ngáº«u nhiÃªn (k=3):
negative_pairs = [
    ("sinh", "trÆ°á»ng"),  # tá»« ngáº«u nhiÃªn 1
    ("sinh", "há»c"),     # tá»« ngáº«u nhiÃªn 2  
    ("sinh", "táº¡i")      # tá»« ngáº«u nhiÃªn 3
]

# Má»¥c tiÃªu:
# Tá»‘i Ä‘a hÃ³a: P("Ä‘i" | "sinh") = sigmoid(u_Ä‘i Â· v_sinh)
# Tá»‘i thiá»ƒu hÃ³a: P("trÆ°á»ng" | "sinh") = sigmoid(u_trÆ°á»ng Â· v_sinh)  
# Tá»‘i thiá»ƒu hÃ³a: P("há»c" | "sinh") = sigmoid(u_há»c Â· v_sinh)
# Tá»‘i thiá»ƒu hÃ³a: P("táº¡i" | "sinh") = sigmoid(u_táº¡i Â· v_sinh)
```

## ğŸš€ Skip-gram trong DeepWalk

### **Chuá»—i Random Walk:**
```python
walks = [
    ["há»c", "sinh", "Ä‘i", "há»c", "táº¡i"],
    ["sinh", "há»c", "trÆ°á»ng", "Ä‘áº¡i"],
    ["Ä‘i", "táº¡i", "trÆ°á»ng", "há»c"]
]
```

### **Chuyá»ƒn Ä‘á»•i thÃ nh dá»¯ liá»‡u huáº¥n luyá»‡n Skip-gram:**
```python
training_pairs = []

for walk in walks:
    for i, center_word in enumerate(walk):
        for j in range(max(0, i-window), min(len(walk), i+window+1)):
            if i != j:
                context_word = walk[j]
                training_pairs.append((center_word, context_word))

# Káº¿t quáº£:
training_pairs = [
    ("há»c", "sinh"), ("há»c", "Ä‘i"),        # tá»« "há»c" táº¡i vá»‹ trÃ­ 0
    ("sinh", "há»c"), ("sinh", "Ä‘i"), ("sinh", "há»c"),  # tá»« "sinh" táº¡i vá»‹ trÃ­ 1
    ("Ä‘i", "há»c"), ("Ä‘i", "sinh"), ("Ä‘i", "há»c"), ("Ä‘i", "táº¡i"), # tá»« "Ä‘i" táº¡i vá»‹ trÃ­ 2
    # ...
]
```

## ğŸ’» Triá»ƒn khai vá»›i Gensim

### **Sá»­ dá»¥ng cÆ¡ báº£n:**
```python
from gensim.models import Word2Vec

# Chuáº©n bá»‹ corpus (danh sÃ¡ch cÃ¡c cÃ¢u Ä‘Ã£ tÃ¡ch tá»«)
corpus = [
    ["há»c", "sinh", "Ä‘i", "há»c", "táº¡i"],
    ["sinh", "há»c", "trÆ°á»ng", "Ä‘áº¡i"],
    ["Ä‘i", "táº¡i", "trÆ°á»ng", "há»c"]
]

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh Skip-gram
model = Word2Vec(
    sentences=corpus,
    vector_size=128,        # sá»‘ chiá»u embedding
    window=5,               # kÃ­ch thÆ°á»›c cá»­a sá»• ngá»¯ cáº£nh  
    min_count=1,           # táº§n suáº¥t tá»« tá»‘i thiá»ƒu
    sg=1,                  # 1=skip-gram, 0=CBOW
    negative=15,           # negative sampling
    epochs=10,             # sá»‘ epoch huáº¥n luyá»‡n
    alpha=0.025,           # tá»‘c Ä‘á»™ há»c ban Ä‘áº§u
    workers=4              # sá»‘ worker song song
)

# Láº¥y embeddings
embedding_há»c = model.wv['há»c']        # numpy array [128,]
similarity = model.wv.similarity('há»c', 'sinh')  # Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine

# TÃ¬m tá»« tÆ°Æ¡ng tá»±
similar_words = model.wv.most_similar('há»c', topn=5)
```

### **Tham sá»‘ nÃ¢ng cao:**
```python
model = Word2Vec(
    sentences=corpus,
    vector_size=300,           # embedding kÃ­ch thÆ°á»›c lá»›n hÆ¡n
    window=10,                 # ngá»¯ cáº£nh rá»™ng hÆ¡n  
    min_count=5,              # lá»c tá»« hiáº¿m
    sg=1,                     # skip-gram
    hs=0,                     # 0=negative sampling, 1=hierarchical softmax
    negative=20,              # nhiá»u negative samples hÆ¡n
    ns_exponent=0.75,         # sá»‘ mÅ© negative sampling
    alpha=0.025,              # tá»‘c Ä‘á»™ há»c
    min_alpha=0.0001,         # tá»‘c Ä‘á»™ há»c tá»‘i thiá»ƒu
    epochs=30,                # nhiá»u epoch huáº¥n luyá»‡n hÆ¡n
    batch_words=10000,        # sá»‘ tá»« má»—i batch
    workers=8,                # xá»­ lÃ½ song song
    callbacks=[callback]      # callback huáº¥n luyá»‡n
)
```

## ğŸ¯ Äiá»u chá»‰nh tham sá»‘ (Hyperparameter Tuning)

### **KÃ­ch thÆ°á»›c Vector:**
- **Nhá»** (50-100): Huáº¥n luyá»‡n nhanh, phÃ¹ há»£p dá»¯ liá»‡u nhá»
- **Trung bÃ¬nh** (100-300): Lá»±a chá»n chuáº©n, cÃ¢n báº±ng tá»‘t
- **Lá»›n** (300-1000): Cháº¥t lÆ°á»£ng tá»‘t hÆ¡n, cáº§n nhiá»u dá»¯ liá»‡u hÆ¡n

### **KÃ­ch thÆ°á»›c cá»­a sá»• ngá»¯ cáº£nh:**
- **Nhá»** (2-5): Náº¯m báº¯t má»‘i quan há»‡ cÃº phÃ¡p  
- **Lá»›n** (10-20): Náº¯m báº¯t má»‘i quan há»‡ ngá»¯ nghÄ©a
- **NguyÃªn táº¯c**: 5-10 cho háº§u háº¿t á»©ng dá»¥ng

### **Negative Sampling:**
- **Nhá»** (5-10): Huáº¥n luyá»‡n nhanh
- **Lá»›n** (15-25): Cháº¥t lÆ°á»£ng tá»‘t hÆ¡n, huáº¥n luyá»‡n cháº­m hÆ¡n
- **CÃ´ng thá»©c**: $k = \text{min}(20, \text{max}(5, \frac{\text{vocab\_\_size}}{10000}))$

### **Tá»‘c Ä‘á»™ há»c (Learning Rate):**
- **Ban Ä‘áº§u**: 0.025 (máº·c Ä‘á»‹nh cá»§a Gensim)
- **Giáº£m dáº§n**: Giáº£m tuyáº¿n tÃ­nh xuá»‘ng min_alpha
- **Alpha tá»‘i thiá»ƒu**: 0.0001

## ğŸ“Š PhÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡

### **ÄÃ¡nh giÃ¡ ná»™i táº¡i (Intrinsic Evaluation):**
1. **Äá»™ tÆ°Æ¡ng Ä‘á»“ng tá»«**: TÆ°Æ¡ng quan vá»›i Ä‘Ã¡nh giÃ¡ cá»§a con ngÆ°á»i
2. **Loáº¡i suy tá»«**: $\mathbf{v}_{\text{vua}} - \mathbf{v}_{\text{Ä‘Ã n Ã´ng}} + \mathbf{v}_{\text{phá»¥ ná»¯}} \approx \mathbf{v}_{\text{ná»¯ hoÃ ng}}$
3. **PhÃ¢n cá»¥m**: K-means trÃªn embeddings

### **ÄÃ¡nh giÃ¡ ngoáº¡i táº¡i (Extrinsic Evaluation):**  
1. **TÃ¡c vá»¥ háº¡ nguá»“n**: PhÃ¢n loáº¡i, nháº­n dáº¡ng thá»±c thá»ƒ, gÃ¡n nhÃ£n tá»« loáº¡i
2. **Truy váº¥n thÃ´ng tin**: Khá»›p query-document
3. **Há»‡ thá»‘ng gá»£i Ã½**: Äá»™ tÆ°Æ¡ng Ä‘á»“ng item

### **VÃ­ dá»¥ Ä‘Ã¡nh giÃ¡:**
```python
# Äá»™ tÆ°Æ¡ng Ä‘á»“ng tá»«
similarity_score = model.wv.similarity('há»c', 'sinh')
print(f"Äá»™ tÆ°Æ¡ng Ä‘á»“ng: {similarity_score:.3f}")

# Loáº¡i suy tá»«
result = model.wv.most_similar(
    positive=['sinh', 'giÃ¡o'], 
    negative=['há»c'], 
    topn=1
)
print(f"há»c : sinh :: giÃ¡o : {result[0][0]}")

# ÄÃ¡nh giÃ¡ phÃ¢n cá»¥m
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

embeddings = [model.wv[word] for word in model.wv.key_to_index]
kmeans = KMeans(n_clusters=10)
labels = kmeans.fit_predict(embeddings)
score = silhouette_score(embeddings, labels)
print(f"Äiá»ƒm silhouette: {score:.3f}")
```

## ğŸ” So sÃ¡nh Skip-gram vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c

| PhÆ°Æ¡ng phÃ¡p | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | TrÆ°á»ng há»£p sá»­ dá»¥ng |
|-------------|---------|------------|-------------------|
| **Skip-gram** | Tá»‘t vá»›i tá»« hiáº¿m, náº¯m báº¯t Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a | TÃ­nh toÃ¡n tá»‘n kÃ©m | Äa nÄƒng, tÃ¡c vá»¥ ngá»¯ nghÄ©a |
| **CBOW** | Huáº¥n luyá»‡n nhanh, tá»‘t vá»›i tá»« phá»• biáº¿n | KÃ©m vá»›i tá»« hiáº¿m | Táº¡o máº«u nhanh, tÃ¡c vá»¥ cÃº phÃ¡p |
| **FastText** | Xá»­ lÃ½ tá»« ngoÃ i tá»« Ä‘iá»ƒn, thÃ´ng tin subword | Phá»©c táº¡p hÆ¡n, mÃ´ hÃ¬nh lá»›n hÆ¡n | NgÃ´n ngá»¯ giÃ u hÃ¬nh thÃ¡i |
| **GloVe** | Thá»‘ng kÃª toÃ n cá»¥c, tÃ­nh xÃ¡c Ä‘á»‹nh | Giáº£ Ä‘á»‹nh má»‘i quan há»‡ tuyáº¿n tÃ­nh | TÃ¡c vá»¥ loáº¡i suy, kháº£ nÄƒng giáº£i thÃ­ch |

## ğŸ“š TÃ i liá»‡u tham kháº£o

### **BÃ i bÃ¡o gá»‘c:**
1. **Word2Vec**: [Mikolov et al., 2013 - Efficient Estimation of Word Representations](https://arxiv.org/abs/1301.3781)
2. **Negative Sampling**: [Mikolov et al., 2013 - Distributed Representations of Words and Phrases](https://arxiv.org/abs/1310.4546)
3. **DeepWalk**: [Perozzi et al., 2014 - DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652)

### **HÆ°á»›ng dáº«n há»c táº­p:**
- [Word2Vec Tutorial - Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
- [The Illustrated Word2Vec - Jay Alammar](https://jalammar.github.io/illustrated-word2vec/)
- [Gensim Word2Vec Tutorial](https://radimrehurek.com/gensim/models/word2vec.html)

### **Chá»§ Ä‘á» nÃ¢ng cao:**
- **Hierarchical Softmax**: Thay tháº¿ cho negative sampling
- **ThÃ´ng tin Subword**: Má»Ÿ rá»™ng FastText
- **Contextualized Embeddings**: PhÃ¡t triá»ƒn BERT, GPT
