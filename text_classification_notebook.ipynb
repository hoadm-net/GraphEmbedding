{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa445ba",
   "metadata": {},
   "source": [
    "# ðŸ“š Text Classification using Graph Embeddings\n",
    "\n",
    "## ðŸŽ¯ Má»¥c tiÃªu\n",
    "PhÃ¢n loáº¡i vÄƒn báº£n tiáº¿ng Viá»‡t báº±ng Graph Embeddings:\n",
    "- **Data**: 10 files (1-5: nÃºi, 6-10: biá»ƒn) + 1 file test\n",
    "- **Method**: Document â†’ Graph â†’ Embeddings â†’ Mean Pooling â†’ KNN\n",
    "- **Goal**: Predict test.txt lÃ  \"nÃºi\" hay \"biá»ƒn\"\n",
    "\n",
    "## ðŸ“‹ Pipeline\n",
    "1. **Load Documents**: Äá»c 10 files training + 1 file test\n",
    "2. **Doc2Graph**: Má»—i document â†’ 1 co-occurrence graph\n",
    "3. **Graph2Embeddings**: DeepWalk cho má»—i graph\n",
    "4. **Graph2Vector**: Mean pooling cá»§a node embeddings\n",
    "5. **Classification**: KNN training vÃ  prediction\n",
    "6. **Visualization**: t-SNE, decision boundaries, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b171f",
   "metadata": {},
   "source": [
    "## âš™ï¸ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aed2d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded successfully!\n",
      "ðŸ“Š Training files: 10, Labels: {'biá»ƒn', 'nÃºi'}\n",
      "ðŸŽ¯ Vector size: 64, KNN k: 3\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset configuration\n",
    "TRAIN_FILES = list(range(1, 11))  # Files 1-10 for training\n",
    "LABELS = {\n",
    "    1: \"nÃºi\", 2: \"nÃºi\", 3: \"nÃºi\", 4: \"nÃºi\", 5: \"nÃºi\",\n",
    "    6: \"biá»ƒn\", 7: \"biá»ƒn\", 8: \"biá»ƒn\", 9: \"biá»ƒn\", 10: \"biá»ƒn\"\n",
    "}\n",
    "TEST_FILE = \"test.txt\"\n",
    "\n",
    "# Graph construction parameters\n",
    "WINDOW_SIZE = 3\n",
    "MIN_FREQUENCY = 1  # Keep all words for small documents\n",
    "WEIGHT_METHOD = \"frequency\"\n",
    "\n",
    "# Random walk parameters\n",
    "WALK_LENGTH = 10\n",
    "NUM_WALKS_PER_NODE = 5\n",
    "\n",
    "# Word2Vec parameters\n",
    "VECTOR_SIZE = 64  # Smaller for demo\n",
    "CONTEXT_WINDOW = 5\n",
    "MIN_COUNT = 1\n",
    "EPOCHS = 20\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Classification parameters\n",
    "KNN_K = 3\n",
    "\n",
    "print(\"âœ… Configuration loaded successfully!\")\n",
    "print(f\"ðŸ“Š Training files: {len(TRAIN_FILES)}, Labels: {set(LABELS.values())}\")\n",
    "print(f\"ðŸŽ¯ Vector size: {VECTOR_SIZE}, KNN k: {KNN_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeae86e",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df5d874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… underthesea imported successfully\n",
      "âœ… ML libraries imported successfully\n",
      "ðŸŽ¯ Setup complete!\n",
      "âœ… ML libraries imported successfully\n",
      "ðŸŽ¯ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Graph processing\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# NLP processing\n",
    "try:\n",
    "    from underthesea import word_tokenize\n",
    "    print(\"âœ… underthesea imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"âŒ underthesea not found. Install: pip install underthesea\")\n",
    "\n",
    "# Machine Learning\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    print(\"âœ… ML libraries imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Missing libraries. Install: pip install gensim scikit-learn\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"ðŸŽ¯ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a9794",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Step 1: Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02216a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading training documents:\n",
      "  ðŸ“„ 1.txt (nÃºi): 1194 characters\n",
      "  ðŸ“„ 2.txt (nÃºi): 1152 characters\n",
      "  ðŸ“„ 3.txt (nÃºi): 1155 characters\n",
      "  ðŸ“„ 4.txt (nÃºi): 1094 characters\n",
      "  ðŸ“„ 5.txt (nÃºi): 1429 characters\n",
      "  ðŸ“„ 6.txt (biá»ƒn): 1898 characters\n",
      "  ðŸ“„ 7.txt (biá»ƒn): 1795 characters\n",
      "  ðŸ“„ 8.txt (biá»ƒn): 2013 characters\n",
      "  ðŸ“„ 9.txt (biá»ƒn): 1973 characters\n",
      "  ðŸ“„ 10.txt (biá»ƒn): 2006 characters\n",
      "\n",
      "ðŸŽ¯ Test document: 2861 characters\n",
      "ðŸ“ Preview: Äá»‰nh Langbiang lÃ  má»™t trong nhá»¯ng ngá»n nÃºi ná»•i tiáº¿ng vÃ  thÆ¡ má»™ng báº­c nháº¥t cá»§a vÃ¹ng TÃ¢y NguyÃªn, thuá»™c...\n",
      "\n",
      "ðŸ“Š Dataset summary:\n",
      "  - Training documents: 10\n",
      "  - Label distribution: Counter({'nÃºi': 5, 'biá»ƒn': 5})\n",
      "  - Test document: âœ…\n"
     ]
    }
   ],
   "source": [
    "# Load training documents\n",
    "data_dir = Path(\"data\")\n",
    "documents = {}\n",
    "labels_list = []\n",
    "\n",
    "print(\"ðŸ“‚ Loading training documents:\")\n",
    "for file_num in TRAIN_FILES:\n",
    "    file_path = data_dir / f\"{file_num}.txt\"\n",
    "    if file_path.exists():\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "            documents[file_num] = content\n",
    "            labels_list.append(LABELS[file_num])\n",
    "            print(f\"  ðŸ“„ {file_num}.txt ({LABELS[file_num]}): {len(content)} characters\")\n",
    "    else:\n",
    "        print(f\"  âŒ {file_path} not found\")\n",
    "\n",
    "# Load test document\n",
    "test_path = data_dir / TEST_FILE\n",
    "test_document = None\n",
    "if test_path.exists():\n",
    "    with open(test_path, 'r', encoding='utf-8') as f:\n",
    "        test_document = f.read().strip()\n",
    "        print(f\"\\nðŸŽ¯ Test document: {len(test_document)} characters\")\n",
    "        print(f\"ðŸ“ Preview: {test_document[:100]}...\")\n",
    "else:\n",
    "    print(f\"\\nâŒ {test_path} not found\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset summary:\")\n",
    "print(f\"  - Training documents: {len(documents)}\")\n",
    "print(f\"  - Label distribution: {Counter(labels_list)}\")\n",
    "print(f\"  - Test document: {'âœ…' if test_document else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb3118",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Step 2: Document to Graph Conversion\n",
    "\n",
    "Táº¡o co-occurrence graph cho má»—i document riÃªng biá»‡t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3a3d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—ï¸ TextGraphBuilder class loaded\n"
     ]
    }
   ],
   "source": [
    "# Reuse TextGraphBuilder from deepwalk_notebook\n",
    "class TextGraphBuilder:\n",
    "    \"\"\"\n",
    "    Build co-occurrence graph from Vietnamese text\n",
    "    \"\"\"\n",
    "    def __init__(self, window_size=WINDOW_SIZE, weight_method=WEIGHT_METHOD):\n",
    "        self.window_size = window_size\n",
    "        self.weight_method = weight_method\n",
    "        self.cooccurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "        self.word_counts = defaultdict(int)\n",
    "        \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Tokenize and clean Vietnamese text\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        cleaned_tokens = []\n",
    "        for token in tokens:\n",
    "            t = token.strip().lower()\n",
    "            if t:\n",
    "                cleaned_tokens.append(t)\n",
    "        return cleaned_tokens\n",
    "    \n",
    "    def build_cooccurrence_matrix(self, tokens):\n",
    "        \"\"\"Build co-occurrence matrix with sliding window\"\"\"\n",
    "        self.cooccurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "        self.word_counts = defaultdict(int)\n",
    "        \n",
    "        for i, center_word in enumerate(tokens):\n",
    "            self.word_counts[center_word] += 1\n",
    "            \n",
    "            start = max(0, i - self.window_size)\n",
    "            end = min(len(tokens), i + self.window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    context_word = tokens[j]\n",
    "                    self.cooccurrence_matrix[center_word][context_word] += 1\n",
    "                    \n",
    "    def build_graph(self, min_frequency=MIN_FREQUENCY):\n",
    "        \"\"\"Create NetworkX graph from co-occurrence matrix\"\"\"\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Filter words by frequency\n",
    "        frequent_words = {\n",
    "            word: count for word, count in self.word_counts.items() \n",
    "            if count >= min_frequency\n",
    "        }\n",
    "        \n",
    "        # Add nodes\n",
    "        for word, count in frequent_words.items():\n",
    "            G.add_node(word, frequency=count)\n",
    "            \n",
    "        # Add edges\n",
    "        for word1 in frequent_words:\n",
    "            for word2, cooccur_count in self.cooccurrence_matrix[word1].items():\n",
    "                if word2 in frequent_words and word1 < word2:\n",
    "                    weight = cooccur_count if self.weight_method == \"frequency\" else 1\n",
    "                    if weight > 0:\n",
    "                        G.add_edge(word1, word2, weight=weight)\n",
    "                        \n",
    "        return G\n",
    "    \n",
    "    def text_to_graph(self, text):\n",
    "        \"\"\"Complete pipeline: text â†’ graph\"\"\"\n",
    "        tokens = self.process_text(text)\n",
    "        self.build_cooccurrence_matrix(tokens)\n",
    "        return self.build_graph()\n",
    "\n",
    "print(\"ðŸ—ï¸ TextGraphBuilder class loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graphs for all documents\n",
    "document_graphs = {}\n",
    "graph_builder = TextGraphBuilder()\n",
    "\n",
    "print(\"ðŸ”„ Building graphs for all documents...\\n\")\n",
    "\n",
    "for file_num, text in documents.items():\n",
    "    print(f\"ðŸ“Š Document {file_num} ({LABELS[file_num]}):\")\n",
    "    \n",
    "    # Build graph\n",
    "    graph = graph_builder.text_to_graph(text)\n",
    "    document_graphs[file_num] = graph\n",
    "    \n",
    "    # Graph statistics\n",
    "    if graph.number_of_nodes() > 0:\n",
    "        # Get largest connected component\n",
    "        if nx.is_connected(graph):\n",
    "            largest_cc = graph\n",
    "        else:\n",
    "            largest_cc = graph.subgraph(max(nx.connected_components(graph), key=len)).copy()\n",
    "        \n",
    "        document_graphs[file_num] = largest_cc  # Use largest component\n",
    "        \n",
    "        print(f\"  - Nodes: {largest_cc.number_of_nodes()}, Edges: {largest_cc.number_of_edges()}\")\n",
    "        print(f\"  - Density: {nx.density(largest_cc):.3f}\")\n",
    "        \n",
    "        # Top frequent words\n",
    "        node_freq = [(node, data['frequency']) for node, data in largest_cc.nodes(data=True)]\n",
    "        top_words = sorted(node_freq, key=lambda x: x[1], reverse=True)[:3]\n",
    "        top_str = \", \".join([f\"{word}({freq})\" for word, freq in top_words])\n",
    "        print(f\"  - Top words: {top_str}\")\n",
    "    else:\n",
    "        print(f\"  - âŒ Empty graph\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Summary\n",
    "valid_graphs = {k: v for k, v in document_graphs.items() if v.number_of_nodes() > 0}\n",
    "print(f\"ðŸ“Š Summary: {len(valid_graphs)}/{len(documents)} documents have valid graphs\")\n",
    "\n",
    "# Graph size distribution\n",
    "graph_sizes = [(file_num, graph.number_of_nodes(), LABELS[file_num]) \n",
    "               for file_num, graph in valid_graphs.items()]\n",
    "df_graphs = pd.DataFrame(graph_sizes, columns=['File', 'Nodes', 'Label'])\n",
    "\n",
    "print(\"\\nðŸ“ˆ Graph size by category:\")\n",
    "print(df_graphs.groupby('Label')['Nodes'].agg(['mean', 'std', 'min', 'max']).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f2480",
   "metadata": {},
   "source": [
    "## ðŸš¶ Step 3: Graph to Embeddings\n",
    "\n",
    "Ãp dá»¥ng DeepWalk (Random Walk + Skip-gram) cho má»—i graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bed634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¶ RandomWalker and DeepWalkEmbedding classes loaded\n"
     ]
    }
   ],
   "source": [
    "# Reuse classes from deepwalk_notebook\n",
    "class RandomWalker:\n",
    "    \"\"\"\n",
    "    Generate random walk sequences from graph\n",
    "    \"\"\"\n",
    "    def __init__(self, graph, walk_length=WALK_LENGTH, num_walks_per_node=NUM_WALKS_PER_NODE):\n",
    "        self.graph = graph\n",
    "        self.walk_length = walk_length\n",
    "        self.num_walks_per_node = num_walks_per_node\n",
    "        \n",
    "    def single_walk(self, start_node):\n",
    "        \"\"\"Perform a single random walk from starting node\"\"\"\n",
    "        if start_node not in self.graph:\n",
    "            return [start_node]\n",
    "            \n",
    "        walk = [start_node]\n",
    "        current_node = start_node\n",
    "        \n",
    "        for _ in range(self.walk_length - 1):\n",
    "            neighbors = list(self.graph.neighbors(current_node))\n",
    "            if not neighbors:\n",
    "                break\n",
    "            next_node = random.choice(neighbors)\n",
    "            walk.append(next_node)\n",
    "            current_node = next_node\n",
    "            \n",
    "        return walk\n",
    "    \n",
    "    def generate_walks(self):\n",
    "        \"\"\"Generate multiple walks from all nodes\"\"\"\n",
    "        nodes = list(self.graph.nodes())\n",
    "        all_walks = []\n",
    "        \n",
    "        for node in nodes:\n",
    "            for _ in range(self.num_walks_per_node):\n",
    "                walk = self.single_walk(node)\n",
    "                if len(walk) > 1:\n",
    "                    all_walks.append(walk)\n",
    "                    \n",
    "        return all_walks\n",
    "\n",
    "class DeepWalkEmbedding:\n",
    "    \"\"\"\n",
    "    DeepWalk implementation using Skip-gram (Word2Vec) on random walks\n",
    "    \"\"\"\n",
    "    def __init__(self, vector_size=VECTOR_SIZE, window=CONTEXT_WINDOW, \n",
    "                 min_count=MIN_COUNT, epochs=EPOCHS):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.epochs = epochs\n",
    "        self.model = None\n",
    "        \n",
    "    def train_on_walks(self, walks):\n",
    "        \"\"\"Train Skip-gram model on random walks\"\"\"\n",
    "        if not walks:\n",
    "            return None\n",
    "            \n",
    "        sentences = [walk for walk in walks if len(walk) > 1]\n",
    "        \n",
    "        if not sentences:\n",
    "            return None\n",
    "            \n",
    "        self.model = Word2Vec(\n",
    "            sentences=sentences,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            sg=1,  # Skip-gram\n",
    "            epochs=self.epochs,\n",
    "            seed=RANDOM_SEED,\n",
    "            workers=1\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \"\"\"Get all node embeddings as dict\"\"\"\n",
    "        if self.model is None:\n",
    "            return {}\n",
    "            \n",
    "        embeddings = {}\n",
    "        for word in self.model.wv.key_to_index:\n",
    "            embeddings[word] = self.model.wv[word]\n",
    "            \n",
    "        return embeddings\n",
    "\n",
    "print(\"ðŸš¶ RandomWalker and DeepWalkEmbedding classes loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8adced9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Generating embeddings for all documents...\n",
      "\n",
      "ðŸ”„ Document 1 (nÃºi):\n",
      "  - Random walks: 695\n",
      "  - Vocabulary: 139 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', '.', 'lÃ ']\n",
      "\n",
      "ðŸ”„ Document 2 (nÃºi):\n",
      "  - Random walks: 725\n",
      "  - Vocabulary: 145 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', '.', 'vÃ ']\n",
      "\n",
      "ðŸ”„ Document 3 (nÃºi):\n",
      "  - Random walks: 695\n",
      "  - Vocabulary: 145 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', '.', 'vÃ ']\n",
      "\n",
      "ðŸ”„ Document 3 (nÃºi):\n",
      "  - Random walks: 695\n",
      "  - Vocabulary: 139 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'lÃ ', '.']\n",
      "\n",
      "ðŸ”„ Document 4 (nÃºi):\n",
      "  - Random walks: 740\n",
      "  - Vocabulary: 148 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', '.', 'vá»›i']\n",
      "\n",
      "ðŸ”„ Document 5 (nÃºi):\n",
      "  - Vocabulary: 139 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'lÃ ', '.']\n",
      "\n",
      "ðŸ”„ Document 4 (nÃºi):\n",
      "  - Random walks: 740\n",
      "  - Vocabulary: 148 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', '.', 'vá»›i']\n",
      "\n",
      "ðŸ”„ Document 5 (nÃºi):\n",
      "  - Random walks: 770\n",
      "  - Random walks: 770\n",
      "  - Vocabulary: 154 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'cá»§a', '.']\n",
      "\n",
      "ðŸ”„ Document 6 (biá»ƒn):\n",
      "  - Random walks: 915\n",
      "  - Vocabulary: 154 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'cá»§a', '.']\n",
      "\n",
      "ðŸ”„ Document 6 (biá»ƒn):\n",
      "  - Random walks: 915\n",
      "  - Vocabulary: 183 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'vÃ ', '.']\n",
      "\n",
      "ðŸ”„ Document 7 (biá»ƒn):\n",
      "  - Random walks: 930\n",
      "  - Vocabulary: 183 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'vÃ ', '.']\n",
      "\n",
      "ðŸ”„ Document 7 (biá»ƒn):\n",
      "  - Random walks: 930\n",
      "  - Vocabulary: 186 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'vÃ ', '.']\n",
      "\n",
      "ðŸ”„ Document 8 (biá»ƒn):\n",
      "  - Random walks: 920\n",
      "  - Vocabulary: 186 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'vÃ ', '.']\n",
      "\n",
      "ðŸ”„ Document 8 (biá»ƒn):\n",
      "  - Random walks: 920\n",
      "  - Vocabulary: 184 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'vÃ ', '.']\n",
      "\n",
      "ðŸ”„ Document 9 (biá»ƒn):\n",
      "  - Random walks: 1020\n",
      "  - Vocabulary: 184 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'vÃ ', '.']\n",
      "\n",
      "ðŸ”„ Document 9 (biá»ƒn):\n",
      "  - Random walks: 1020\n",
      "  - Vocabulary: 204 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'vÃ ', '.']\n",
      "\n",
      "ðŸ”„ Document 10 (biá»ƒn):\n",
      "  - Random walks: 1025\n",
      "  - Vocabulary: 204 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'vÃ ', '.']\n",
      "\n",
      "ðŸ”„ Document 10 (biá»ƒn):\n",
      "  - Random walks: 1025\n",
      "  - Vocabulary: 205 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'vÃ ', '.']\n",
      "\n",
      "ðŸ“Š Successfully generated embeddings for 10 documents\n",
      "  - Vocabulary: 205 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'vÃ ', '.']\n",
      "\n",
      "ðŸ“Š Successfully generated embeddings for 10 documents\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all document graphs\n",
    "document_embeddings = {}\n",
    "\n",
    "print(\"ðŸ§  Generating embeddings for all documents...\\n\")\n",
    "\n",
    "for file_num, graph in valid_graphs.items():\n",
    "    print(f\"ðŸ”„ Document {file_num} ({LABELS[file_num]}):\")\n",
    "    \n",
    "    # Generate random walks\n",
    "    walker = RandomWalker(graph)\n",
    "    walks = walker.generate_walks()\n",
    "    \n",
    "    print(f\"  - Random walks: {len(walks)}\")\n",
    "    \n",
    "    if walks:\n",
    "        # Train embeddings\n",
    "        deepwalk = DeepWalkEmbedding()\n",
    "        model = deepwalk.train_on_walks(walks)\n",
    "        \n",
    "        if model is not None:\n",
    "            embeddings = deepwalk.get_embeddings()\n",
    "            document_embeddings[file_num] = embeddings\n",
    "            \n",
    "            print(f\"  - Vocabulary: {len(embeddings)} words\")\n",
    "            print(f\"  - Vector size: {VECTOR_SIZE}\")\n",
    "            \n",
    "            # Sample words\n",
    "            sample_words = list(embeddings.keys())[:3]\n",
    "            print(f\"  - Sample words: {sample_words}\")\n",
    "        else:\n",
    "            print(f\"  - âŒ Failed to train embeddings\")\n",
    "    else:\n",
    "        print(f\"  - âŒ No walks generated\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"ðŸ“Š Successfully generated embeddings for {len(document_embeddings)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b70524",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 4: Document Vector Representation\n",
    "\n",
    "Chuyá»ƒn Ä‘á»•i node embeddings thÃ nh document vectors báº±ng mean pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31452d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Creating document vectors using mean pooling...\n",
      "\n",
      "ðŸ”„ Process: Graph â†’ Node Embeddings â†’ Document Vector (Mean Pooling)\n",
      "============================================================\n",
      "ðŸ“„ Document 1 (nÃºi):\n",
      "  - Input: 139 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling â†’ Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.505\n",
      "\n",
      "ðŸ“„ Document 2 (nÃºi):\n",
      "  - Input: 145 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling â†’ Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.558\n",
      "\n",
      "ðŸ“„ Document 3 (nÃºi):\n",
      "  - Input: 139 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling â†’ Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.486\n",
      "\n",
      "ðŸ“„ Document 4 (nÃºi):\n",
      "  - Input: 148 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling â†’ Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.575\n",
      "\n",
      "ðŸ“„ Document 5 (nÃºi):\n",
      "  - Input: 154 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling â†’ Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.456\n",
      "\n",
      "ðŸ“„ Document 6 (biá»ƒn):\n",
      "  - Input: 183 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling â†’ Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.462\n",
      "\n",
      "ðŸ“„ Document 7 (biá»ƒn):\n",
      "  - Input: 186 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling â†’ Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.516\n",
      "\n",
      "ðŸ“„ Document 8 (biá»ƒn):\n",
      "  - Input: 184 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling â†’ Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.441\n",
      "\n",
      "ðŸ“„ Document 9 (biá»ƒn):\n",
      "  - Input: 204 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling â†’ Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.528\n",
      "\n",
      "ðŸ“„ Document 10 (biá»ƒn):\n",
      "  - Input: 205 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling â†’ Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.482\n",
      "\n",
      "ðŸ“Š Final training dataset:\n",
      "  - Shape: (10, 64) (documents Ã— features)\n",
      "  - Labels: Counter({'nÃºi': 5, 'biá»ƒn': 5})\n",
      "  - Files: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def create_document_vector(embeddings_dict, method='mean'):\n",
    "    \"\"\"\n",
    "    ðŸŽ¯ CORE FUNCTION: Táº¡o vector Ä‘áº¡i diá»‡n cho document tá»« node embeddings\n",
    "    \n",
    "    INPUT: embeddings_dict = {word1: vector1, word2: vector2, ...}\n",
    "    OUTPUT: single vector Ä‘áº¡i diá»‡n cho toÃ n bá»™ document\n",
    "    \n",
    "    METHOD: Mean pooling - láº¥y trung bÃ¬nh cá»§a táº¥t cáº£ node embeddings\n",
    "    \"\"\"\n",
    "    if not embeddings_dict:\n",
    "        return np.zeros(VECTOR_SIZE)\n",
    "    \n",
    "    vectors = list(embeddings_dict.values())\n",
    "    \n",
    "    if method == 'mean':\n",
    "        # Mean pooling: vector trung bÃ¬nh cá»§a táº¥t cáº£ nodes\n",
    "        return np.mean(vectors, axis=0)\n",
    "    elif method == 'sum':\n",
    "        return np.sum(vectors, axis=0)\n",
    "    elif method == 'max':\n",
    "        return np.max(vectors, axis=0)\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "# ========================================================================\n",
    "# ðŸŽ¯ BÆ¯á»šC QUAN TRá»ŒNG: Táº¡o document vectors tá»« graph embeddings\n",
    "# ========================================================================\n",
    "print(\"ðŸ“Š Creating document vectors using mean pooling...\\n\")\n",
    "print(\"ðŸ”„ Process: Graph â†’ Node Embeddings â†’ Document Vector (Mean Pooling)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "document_vectors = {}\n",
    "X = []  # Training features\n",
    "y = []  # Training labels  \n",
    "file_nums = []\n",
    "\n",
    "for file_num in sorted(document_embeddings.keys()):\n",
    "    embeddings = document_embeddings[file_num]\n",
    "    \n",
    "    # ðŸŽ¯ CORE: Chuyá»ƒn tá»« node embeddings thÃ nh 1 vector cho document\n",
    "    doc_vector = create_document_vector(embeddings, method='mean')\n",
    "    document_vectors[file_num] = doc_vector\n",
    "    \n",
    "    # LÆ°u vÃ o training set\n",
    "    X.append(doc_vector)\n",
    "    y.append(LABELS[file_num])\n",
    "    file_nums.append(file_num)\n",
    "    \n",
    "    print(f\"ðŸ“„ Document {file_num} ({LABELS[file_num]}):\")\n",
    "    print(f\"  - Input: {len(embeddings)} node embeddings (size {VECTOR_SIZE} each)\")\n",
    "    print(f\"  - Process: Mean pooling â†’ Single document vector\")\n",
    "    print(f\"  - Output: Document vector shape {doc_vector.shape}, norm: {np.linalg.norm(doc_vector):.3f}\")\n",
    "    print()\n",
    "\n",
    "# Convert to numpy arrays for machine learning\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"ðŸ“Š Final training dataset:\")\n",
    "print(f\"  - Shape: {X.shape} (documents Ã— features)\")\n",
    "print(f\"  - Labels: {Counter(y)}\")\n",
    "print(f\"  - Files: {file_nums}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3814b4a1",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 5: Classification with KNN\n",
    "\n",
    "Huáº¥n luyá»‡n mÃ´ hÃ¬nh KNN vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN classifier on all 10 training documents\n",
    "print(\"ðŸŽ¯ Training KNN classifier on full dataset...\\n\")\n",
    "\n",
    "# Initialize classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=KNN_K, metric='cosine')\n",
    "\n",
    "# Train on full dataset\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Get training set predictions (for reference)\n",
    "y_pred_train = knn.predict(X)\n",
    "y_proba_train = knn.predict_proba(X)\n",
    "\n",
    "print(f\"ðŸ“Š Training dataset overview:\")\n",
    "print(f\"  - Total samples: {len(X)}\")\n",
    "print(f\"  - Features: {X.shape[1]}\")\n",
    "print(f\"  - Classes: {list(knn.classes_)}\")\n",
    "print(f\"  - KNN k: {KNN_K}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Training set predictions (for reference):\")\n",
    "for i, (file_num, true_label, pred_label) in enumerate(zip(file_nums, y, y_pred_train)):\n",
    "    status = \"âœ…\" if true_label == pred_label else \"âŒ\"\n",
    "    confidence = max(y_proba_train[i])\n",
    "    print(f\"  {status} File {file_num}: {true_label} â†’ {pred_label} (conf: {confidence:.3f})\")\n",
    "\n",
    "train_accuracy = accuracy_score(y, y_pred_train)\n",
    "print(f\"\\nðŸŽ¯ Training accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"ðŸ“ Note: High training accuracy is expected (overfitting on small dataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aaf9fc",
   "metadata": {},
   "source": [
    "## ðŸ”® Step 6: Test Prediction\n",
    "\n",
    "Dá»± Ä‘oÃ¡n nhÃ£n cho file test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ðŸ”® BÆ¯á»šC Dá»° ÄOÃN: Ãp dá»¥ng pipeline cho test.txt\n",
    "# ========================================================================\n",
    "print(\"ðŸ”® PREDICTION PIPELINE FOR TEST.TXT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ”„ Process: test.txt â†’ Graph â†’ Embeddings â†’ Vector â†’ KNN Prediction\")\n",
    "print()\n",
    "\n",
    "if test_document:\n",
    "    print(\"ðŸ”® Processing test document...\")\n",
    "    \n",
    "    # STEP 1: Text â†’ Graph\n",
    "    print(\"\\nðŸ“Š STEP 1: Building graph for test document\")\n",
    "    test_graph = graph_builder.text_to_graph(test_document)\n",
    "    \n",
    "    print(f\"  - Nodes: {test_graph.number_of_nodes()}\")\n",
    "    print(f\"  - Edges: {test_graph.number_of_edges()}\")\n",
    "    \n",
    "    if test_graph.number_of_nodes() > 0:\n",
    "        # Get largest connected component\n",
    "        if nx.is_connected(test_graph):\n",
    "            test_graph_main = test_graph\n",
    "        else:\n",
    "            test_graph_main = test_graph.subgraph(max(nx.connected_components(test_graph), key=len)).copy()\n",
    "        \n",
    "        print(f\"  - Main component: {test_graph_main.number_of_nodes()} nodes\")\n",
    "        \n",
    "        # STEP 2: Graph â†’ Embeddings  \n",
    "        print(\"\\nðŸš¶ STEP 2: Generating embeddings for test graph\")\n",
    "        test_walker = RandomWalker(test_graph_main)\n",
    "        test_walks = test_walker.generate_walks()\n",
    "        \n",
    "        print(f\"  - Random walks: {len(test_walks)}\")\n",
    "        \n",
    "        if test_walks:\n",
    "            test_deepwalk = DeepWalkEmbedding()\n",
    "            test_model = test_deepwalk.train_on_walks(test_walks)\n",
    "            \n",
    "            if test_model is not None:\n",
    "                test_embeddings = test_deepwalk.get_embeddings()\n",
    "                \n",
    "                print(f\"  - Vocabulary: {len(test_embeddings)} words\")\n",
    "                \n",
    "                # STEP 3: Embeddings â†’ Document Vector\n",
    "                print(\"\\nðŸ“Š STEP 3: Creating document vector (mean pooling)\")\n",
    "                test_vector = create_document_vector(test_embeddings, method='mean')\n",
    "                \n",
    "                print(f\"  - Input: {len(test_embeddings)} node embeddings\")\n",
    "                print(f\"  - Output: Document vector norm: {np.linalg.norm(test_vector):.3f}\")\n",
    "                \n",
    "                # STEP 4: Vector â†’ Prediction\n",
    "                print(\"\\nðŸŽ¯ STEP 4: KNN PREDICTION\")\n",
    "                prediction = knn.predict([test_vector])[0]\n",
    "                probabilities = knn.predict_proba([test_vector])[0]\n",
    "                \n",
    "                print(\"=\" * 40)\n",
    "                print(\"ðŸŽ¯ FINAL PREDICTION RESULTS:\")\n",
    "                print(\"=\" * 40)\n",
    "                print(f\"ðŸ“„ Test document: {TEST_FILE}\")\n",
    "                print(f\"ðŸ”® Predicted class: **{prediction}**\")\n",
    "                print(f\"ðŸ“Š Confidence scores:\")\n",
    "                \n",
    "                class_names = knn.classes_\n",
    "                for class_name, prob in zip(class_names, probabilities):\n",
    "                    confidence_bar = \"â–ˆ\" * int(prob * 20)\n",
    "                    print(f\"     - {class_name}: {prob:.3f} {confidence_bar}\")\n",
    "                \n",
    "                # Nearest neighbors analysis\n",
    "                distances, indices = knn.kneighbors([test_vector])\n",
    "                print(f\"\\nðŸ” Nearest neighbors (why this prediction?):\")\n",
    "                for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "                    neighbor_file = file_nums[idx]\n",
    "                    neighbor_label = y[idx]\n",
    "                    print(f\"     {i+1}. File {neighbor_file} ({neighbor_label}): distance = {dist:.3f}\")\n",
    "                \n",
    "                print(\"=\" * 40)\n",
    "                \n",
    "            else:\n",
    "                print(\"  âŒ Failed to train embeddings for test document\")\n",
    "        else:\n",
    "            print(\"  âŒ No walks generated for test document\")\n",
    "    else:\n",
    "        print(\"  âŒ Empty graph for test document\")\n",
    "else:\n",
    "    print(\"âŒ No test document available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d8ae8",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 7: Visualization\n",
    "\n",
    "Trá»±c quan hÃ³a 11 document vectors (10 training + 1 test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b37e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ðŸ“Š VISUALIZATION: 11 Document Vectors (10 training + 1 test)\n",
    "# ========================================================================\n",
    "print(\"ðŸŽ¨ CREATING COMPREHENSIVE VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š Showing all 11 documents: 10 training + 1 test\")\n",
    "print()\n",
    "\n",
    "if len(X) >= 4:  # Need minimum samples for t-SNE\n",
    "    print(\"ðŸŽ¨ Creating t-SNE visualization for all 11 documents...\")\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    X_viz = X.copy()\n",
    "    y_viz = y.copy()\n",
    "    files_viz = file_nums.copy()\n",
    "    \n",
    "    # Add test document if available\n",
    "    if 'test_vector' in locals():\n",
    "        X_viz = np.vstack([X_viz, test_vector])\n",
    "        y_viz = np.append(y_viz, f\"test_{prediction}\")  # Show predicted class\n",
    "        files_viz.append(\"TEST\")\n",
    "        print(f\"âœ… Added test document with predicted label: {prediction}\")\n",
    "    \n",
    "    print(f\"ðŸ“Š Total documents for visualization: {len(X_viz)}\")\n",
    "    print(f\"   - Training: {len(X)} documents\")\n",
    "    print(f\"   - Test: 1 document\")\n",
    "    print()\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    perplexity = min(5, len(X_viz) - 1)\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=RANDOM_SEED, max_iter=1000)\n",
    "    X_2d = tsne.fit_transform(X_viz)\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot 1: All documents with labels\n",
    "    unique_labels = list(set(y_viz))\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "    color_map = dict(zip(unique_labels, colors))\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        mask = y_viz == label\n",
    "        marker = 'o' if 'test' not in label else '*'  # Star for test document\n",
    "        size = 100 if 'test' not in label else 300  # Larger for test\n",
    "        ax1.scatter(X_2d[mask, 0], X_2d[mask, 1], \n",
    "                   c=[color_map[label]], label=label, s=size, alpha=0.7, marker=marker,\n",
    "                   edgecolors='black' if 'test' in label else 'none', linewidths=2)\n",
    "    \n",
    "    # Add file labels\n",
    "    for i, file_id in enumerate(files_viz):\n",
    "        ax1.annotate(str(file_id), (X_2d[i, 0], X_2d[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points', \n",
    "                    fontsize=10, alpha=0.8, fontweight='bold' if file_id == 'TEST' else 'normal')\n",
    "    \n",
    "    ax1.set_title('ðŸ“Š All 11 Documents: Training + Test (t-SNE)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('t-SNE Component 1')\n",
    "    ax1.set_ylabel('t-SNE Component 2')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Focus on classification with nearest neighbors\n",
    "    training_mask = y_viz != f\"test_{prediction}\"\n",
    "    test_mask = y_viz == f\"test_{prediction}\"\n",
    "    \n",
    "    # Plot training points by true label\n",
    "    for label in ['nÃºi', 'biá»ƒn']:\n",
    "        label_mask = y_viz == label\n",
    "        ax2.scatter(X_2d[label_mask, 0], X_2d[label_mask, 1], \n",
    "                   c=[color_map[label]], s=100, alpha=0.7, label=f'Training: {label}')\n",
    "    \n",
    "    # Plot test point\n",
    "    if np.any(test_mask):\n",
    "        ax2.scatter(X_2d[test_mask, 0], X_2d[test_mask, 1], \n",
    "                   c=[color_map[y_viz[test_mask][0]]], s=300, marker='*', \n",
    "                   edgecolors='black', linewidths=2, label=f'Test â†’ {prediction}', zorder=10)\n",
    "        \n",
    "        # Draw lines to nearest neighbors\n",
    "        test_idx = np.where(test_mask)[0][0]\n",
    "        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "            ax2.plot([X_2d[test_idx, 0], X_2d[idx, 0]], \n",
    "                    [X_2d[test_idx, 1], X_2d[idx, 1]], \n",
    "                    'k--', alpha=0.3, linewidth=1)\n",
    "            # Add distance annotation\n",
    "            mid_x = (X_2d[test_idx, 0] + X_2d[idx, 0]) / 2\n",
    "            mid_y = (X_2d[test_idx, 1] + X_2d[idx, 1]) / 2\n",
    "            ax2.annotate(f'd={dist:.2f}', (mid_x, mid_y), \n",
    "                        fontsize=8, alpha=0.6, ha='center')\n",
    "    \n",
    "    # Add file labels\n",
    "    for i, file_id in enumerate(files_viz):\n",
    "        color = 'red' if file_id == 'TEST' else 'black'\n",
    "        weight = 'bold' if file_id == 'TEST' else 'normal'\n",
    "        ax2.annotate(str(file_id), (X_2d[i, 0], X_2d[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points', \n",
    "                    fontsize=10, alpha=0.8, color=color, fontweight=weight)\n",
    "    \n",
    "    ax2.set_title('ðŸŽ¯ KNN Classification: Test with Nearest Neighbors', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('t-SNE Component 1')\n",
    "    ax2.set_ylabel('t-SNE Component 2')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nðŸ“Š VISUALIZATION SUMMARY:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"ðŸ“ˆ Total points visualized: {len(X_viz)}\")\n",
    "    print(f\"ðŸŽ¯ Test document: {TEST_FILE}\")\n",
    "    print(f\"ðŸ”® Predicted as: {prediction}\")\n",
    "    print(f\"ðŸ“ Test point marked with â­ (star)\")\n",
    "    print(f\"ðŸ”— Dashed lines show k={KNN_K} nearest neighbors\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Insufficient data for t-SNE visualization (need >= 4 samples)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
