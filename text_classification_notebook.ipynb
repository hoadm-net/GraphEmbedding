{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa445ba",
   "metadata": {},
   "source": [
    "# 📚 Text Classification using Graph Embeddings\n",
    "\n",
    "## 🎯 Mục tiêu\n",
    "Phân loại văn bản tiếng Việt bằng Graph Embeddings:\n",
    "- **Data**: 10 files (1-5: núi, 6-10: biển) + 1 file test\n",
    "- **Method**: Document → Graph → Embeddings → Mean Pooling → KNN\n",
    "- **Goal**: Predict test.txt là \"núi\" hay \"biển\"\n",
    "\n",
    "## 📋 Pipeline\n",
    "1. **Load Documents**: Đọc 10 files training + 1 file test\n",
    "2. **Doc2Graph**: Mỗi document → 1 co-occurrence graph\n",
    "3. **Graph2Embeddings**: DeepWalk cho mỗi graph\n",
    "4. **Graph2Vector**: Mean pooling của node embeddings\n",
    "5. **Classification**: KNN training và prediction\n",
    "6. **Visualization**: t-SNE, decision boundaries, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b171f",
   "metadata": {},
   "source": [
    "## ⚙️ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aed2d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded successfully!\n",
      "📊 Training files: 10, Labels: {'biển', 'núi'}\n",
      "🎯 Vector size: 64, KNN k: 3\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset configuration\n",
    "TRAIN_FILES = list(range(1, 11))  # Files 1-10 for training\n",
    "LABELS = {\n",
    "    1: \"núi\", 2: \"núi\", 3: \"núi\", 4: \"núi\", 5: \"núi\",\n",
    "    6: \"biển\", 7: \"biển\", 8: \"biển\", 9: \"biển\", 10: \"biển\"\n",
    "}\n",
    "TEST_FILE = \"test.txt\"\n",
    "\n",
    "# Graph construction parameters\n",
    "WINDOW_SIZE = 3\n",
    "MIN_FREQUENCY = 1  # Keep all words for small documents\n",
    "WEIGHT_METHOD = \"frequency\"\n",
    "\n",
    "# Random walk parameters\n",
    "WALK_LENGTH = 10\n",
    "NUM_WALKS_PER_NODE = 5\n",
    "\n",
    "# Word2Vec parameters\n",
    "VECTOR_SIZE = 64  # Smaller for demo\n",
    "CONTEXT_WINDOW = 5\n",
    "MIN_COUNT = 1\n",
    "EPOCHS = 20\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Classification parameters\n",
    "KNN_K = 3\n",
    "\n",
    "print(\"✅ Configuration loaded successfully!\")\n",
    "print(f\"📊 Training files: {len(TRAIN_FILES)}, Labels: {set(LABELS.values())}\")\n",
    "print(f\"🎯 Vector size: {VECTOR_SIZE}, KNN k: {KNN_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeae86e",
   "metadata": {},
   "source": [
    "## 📦 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df5d874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ underthesea imported successfully\n",
      "✅ ML libraries imported successfully\n",
      "🎯 Setup complete!\n",
      "✅ ML libraries imported successfully\n",
      "🎯 Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Graph processing\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# NLP processing\n",
    "try:\n",
    "    from underthesea import word_tokenize\n",
    "    print(\"✅ underthesea imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"❌ underthesea not found. Install: pip install underthesea\")\n",
    "\n",
    "# Machine Learning\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    print(\"✅ ML libraries imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"❌ Missing libraries. Install: pip install gensim scikit-learn\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"🎯 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a9794",
   "metadata": {},
   "source": [
    "## 📂 Step 1: Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02216a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading training documents:\n",
      "  📄 1.txt (núi): 1194 characters\n",
      "  📄 2.txt (núi): 1152 characters\n",
      "  📄 3.txt (núi): 1155 characters\n",
      "  📄 4.txt (núi): 1094 characters\n",
      "  📄 5.txt (núi): 1429 characters\n",
      "  📄 6.txt (biển): 1898 characters\n",
      "  📄 7.txt (biển): 1795 characters\n",
      "  📄 8.txt (biển): 2013 characters\n",
      "  📄 9.txt (biển): 1973 characters\n",
      "  📄 10.txt (biển): 2006 characters\n",
      "\n",
      "🎯 Test document: 2861 characters\n",
      "📝 Preview: Đỉnh Langbiang là một trong những ngọn núi nổi tiếng và thơ mộng bậc nhất của vùng Tây Nguyên, thuộc...\n",
      "\n",
      "📊 Dataset summary:\n",
      "  - Training documents: 10\n",
      "  - Label distribution: Counter({'núi': 5, 'biển': 5})\n",
      "  - Test document: ✅\n"
     ]
    }
   ],
   "source": [
    "# Load training documents\n",
    "data_dir = Path(\"data\")\n",
    "documents = {}\n",
    "labels_list = []\n",
    "\n",
    "print(\"📂 Loading training documents:\")\n",
    "for file_num in TRAIN_FILES:\n",
    "    file_path = data_dir / f\"{file_num}.txt\"\n",
    "    if file_path.exists():\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "            documents[file_num] = content\n",
    "            labels_list.append(LABELS[file_num])\n",
    "            print(f\"  📄 {file_num}.txt ({LABELS[file_num]}): {len(content)} characters\")\n",
    "    else:\n",
    "        print(f\"  ❌ {file_path} not found\")\n",
    "\n",
    "# Load test document\n",
    "test_path = data_dir / TEST_FILE\n",
    "test_document = None\n",
    "if test_path.exists():\n",
    "    with open(test_path, 'r', encoding='utf-8') as f:\n",
    "        test_document = f.read().strip()\n",
    "        print(f\"\\n🎯 Test document: {len(test_document)} characters\")\n",
    "        print(f\"📝 Preview: {test_document[:100]}...\")\n",
    "else:\n",
    "    print(f\"\\n❌ {test_path} not found\")\n",
    "\n",
    "print(f\"\\n📊 Dataset summary:\")\n",
    "print(f\"  - Training documents: {len(documents)}\")\n",
    "print(f\"  - Label distribution: {Counter(labels_list)}\")\n",
    "print(f\"  - Test document: {'✅' if test_document else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb3118",
   "metadata": {},
   "source": [
    "## 🏗️ Step 2: Document to Graph Conversion\n",
    "\n",
    "Tạo co-occurrence graph cho mỗi document riêng biệt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3a3d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ TextGraphBuilder class loaded\n"
     ]
    }
   ],
   "source": [
    "# Reuse TextGraphBuilder from deepwalk_notebook\n",
    "class TextGraphBuilder:\n",
    "    \"\"\"\n",
    "    Build co-occurrence graph from Vietnamese text\n",
    "    \"\"\"\n",
    "    def __init__(self, window_size=WINDOW_SIZE, weight_method=WEIGHT_METHOD):\n",
    "        self.window_size = window_size\n",
    "        self.weight_method = weight_method\n",
    "        self.cooccurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "        self.word_counts = defaultdict(int)\n",
    "        \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Tokenize and clean Vietnamese text\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        cleaned_tokens = []\n",
    "        for token in tokens:\n",
    "            t = token.strip().lower()\n",
    "            if t:\n",
    "                cleaned_tokens.append(t)\n",
    "        return cleaned_tokens\n",
    "    \n",
    "    def build_cooccurrence_matrix(self, tokens):\n",
    "        \"\"\"Build co-occurrence matrix with sliding window\"\"\"\n",
    "        self.cooccurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "        self.word_counts = defaultdict(int)\n",
    "        \n",
    "        for i, center_word in enumerate(tokens):\n",
    "            self.word_counts[center_word] += 1\n",
    "            \n",
    "            start = max(0, i - self.window_size)\n",
    "            end = min(len(tokens), i + self.window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    context_word = tokens[j]\n",
    "                    self.cooccurrence_matrix[center_word][context_word] += 1\n",
    "                    \n",
    "    def build_graph(self, min_frequency=MIN_FREQUENCY):\n",
    "        \"\"\"Create NetworkX graph from co-occurrence matrix\"\"\"\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Filter words by frequency\n",
    "        frequent_words = {\n",
    "            word: count for word, count in self.word_counts.items() \n",
    "            if count >= min_frequency\n",
    "        }\n",
    "        \n",
    "        # Add nodes\n",
    "        for word, count in frequent_words.items():\n",
    "            G.add_node(word, frequency=count)\n",
    "            \n",
    "        # Add edges\n",
    "        for word1 in frequent_words:\n",
    "            for word2, cooccur_count in self.cooccurrence_matrix[word1].items():\n",
    "                if word2 in frequent_words and word1 < word2:\n",
    "                    weight = cooccur_count if self.weight_method == \"frequency\" else 1\n",
    "                    if weight > 0:\n",
    "                        G.add_edge(word1, word2, weight=weight)\n",
    "                        \n",
    "        return G\n",
    "    \n",
    "    def text_to_graph(self, text):\n",
    "        \"\"\"Complete pipeline: text → graph\"\"\"\n",
    "        tokens = self.process_text(text)\n",
    "        self.build_cooccurrence_matrix(tokens)\n",
    "        return self.build_graph()\n",
    "\n",
    "print(\"🏗️ TextGraphBuilder class loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graphs for all documents\n",
    "document_graphs = {}\n",
    "graph_builder = TextGraphBuilder()\n",
    "\n",
    "print(\"🔄 Building graphs for all documents...\\n\")\n",
    "\n",
    "for file_num, text in documents.items():\n",
    "    print(f\"📊 Document {file_num} ({LABELS[file_num]}):\")\n",
    "    \n",
    "    # Build graph\n",
    "    graph = graph_builder.text_to_graph(text)\n",
    "    document_graphs[file_num] = graph\n",
    "    \n",
    "    # Graph statistics\n",
    "    if graph.number_of_nodes() > 0:\n",
    "        # Get largest connected component\n",
    "        if nx.is_connected(graph):\n",
    "            largest_cc = graph\n",
    "        else:\n",
    "            largest_cc = graph.subgraph(max(nx.connected_components(graph), key=len)).copy()\n",
    "        \n",
    "        document_graphs[file_num] = largest_cc  # Use largest component\n",
    "        \n",
    "        print(f\"  - Nodes: {largest_cc.number_of_nodes()}, Edges: {largest_cc.number_of_edges()}\")\n",
    "        print(f\"  - Density: {nx.density(largest_cc):.3f}\")\n",
    "        \n",
    "        # Top frequent words\n",
    "        node_freq = [(node, data['frequency']) for node, data in largest_cc.nodes(data=True)]\n",
    "        top_words = sorted(node_freq, key=lambda x: x[1], reverse=True)[:3]\n",
    "        top_str = \", \".join([f\"{word}({freq})\" for word, freq in top_words])\n",
    "        print(f\"  - Top words: {top_str}\")\n",
    "    else:\n",
    "        print(f\"  - ❌ Empty graph\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Summary\n",
    "valid_graphs = {k: v for k, v in document_graphs.items() if v.number_of_nodes() > 0}\n",
    "print(f\"📊 Summary: {len(valid_graphs)}/{len(documents)} documents have valid graphs\")\n",
    "\n",
    "# Graph size distribution\n",
    "graph_sizes = [(file_num, graph.number_of_nodes(), LABELS[file_num]) \n",
    "               for file_num, graph in valid_graphs.items()]\n",
    "df_graphs = pd.DataFrame(graph_sizes, columns=['File', 'Nodes', 'Label'])\n",
    "\n",
    "print(\"\\n📈 Graph size by category:\")\n",
    "print(df_graphs.groupby('Label')['Nodes'].agg(['mean', 'std', 'min', 'max']).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f2480",
   "metadata": {},
   "source": [
    "## 🚶 Step 3: Graph to Embeddings\n",
    "\n",
    "Áp dụng DeepWalk (Random Walk + Skip-gram) cho mỗi graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bed634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚶 RandomWalker and DeepWalkEmbedding classes loaded\n"
     ]
    }
   ],
   "source": [
    "# Reuse classes from deepwalk_notebook\n",
    "class RandomWalker:\n",
    "    \"\"\"\n",
    "    Generate random walk sequences from graph\n",
    "    \"\"\"\n",
    "    def __init__(self, graph, walk_length=WALK_LENGTH, num_walks_per_node=NUM_WALKS_PER_NODE):\n",
    "        self.graph = graph\n",
    "        self.walk_length = walk_length\n",
    "        self.num_walks_per_node = num_walks_per_node\n",
    "        \n",
    "    def single_walk(self, start_node):\n",
    "        \"\"\"Perform a single random walk from starting node\"\"\"\n",
    "        if start_node not in self.graph:\n",
    "            return [start_node]\n",
    "            \n",
    "        walk = [start_node]\n",
    "        current_node = start_node\n",
    "        \n",
    "        for _ in range(self.walk_length - 1):\n",
    "            neighbors = list(self.graph.neighbors(current_node))\n",
    "            if not neighbors:\n",
    "                break\n",
    "            next_node = random.choice(neighbors)\n",
    "            walk.append(next_node)\n",
    "            current_node = next_node\n",
    "            \n",
    "        return walk\n",
    "    \n",
    "    def generate_walks(self):\n",
    "        \"\"\"Generate multiple walks from all nodes\"\"\"\n",
    "        nodes = list(self.graph.nodes())\n",
    "        all_walks = []\n",
    "        \n",
    "        for node in nodes:\n",
    "            for _ in range(self.num_walks_per_node):\n",
    "                walk = self.single_walk(node)\n",
    "                if len(walk) > 1:\n",
    "                    all_walks.append(walk)\n",
    "                    \n",
    "        return all_walks\n",
    "\n",
    "class DeepWalkEmbedding:\n",
    "    \"\"\"\n",
    "    DeepWalk implementation using Skip-gram (Word2Vec) on random walks\n",
    "    \"\"\"\n",
    "    def __init__(self, vector_size=VECTOR_SIZE, window=CONTEXT_WINDOW, \n",
    "                 min_count=MIN_COUNT, epochs=EPOCHS):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.epochs = epochs\n",
    "        self.model = None\n",
    "        \n",
    "    def train_on_walks(self, walks):\n",
    "        \"\"\"Train Skip-gram model on random walks\"\"\"\n",
    "        if not walks:\n",
    "            return None\n",
    "            \n",
    "        sentences = [walk for walk in walks if len(walk) > 1]\n",
    "        \n",
    "        if not sentences:\n",
    "            return None\n",
    "            \n",
    "        self.model = Word2Vec(\n",
    "            sentences=sentences,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            sg=1,  # Skip-gram\n",
    "            epochs=self.epochs,\n",
    "            seed=RANDOM_SEED,\n",
    "            workers=1\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \"\"\"Get all node embeddings as dict\"\"\"\n",
    "        if self.model is None:\n",
    "            return {}\n",
    "            \n",
    "        embeddings = {}\n",
    "        for word in self.model.wv.key_to_index:\n",
    "            embeddings[word] = self.model.wv[word]\n",
    "            \n",
    "        return embeddings\n",
    "\n",
    "print(\"🚶 RandomWalker and DeepWalkEmbedding classes loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8adced9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Generating embeddings for all documents...\n",
      "\n",
      "🔄 Document 1 (núi):\n",
      "  - Random walks: 695\n",
      "  - Vocabulary: 139 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', '.', 'là']\n",
      "\n",
      "🔄 Document 2 (núi):\n",
      "  - Random walks: 725\n",
      "  - Vocabulary: 145 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', '.', 'và']\n",
      "\n",
      "🔄 Document 3 (núi):\n",
      "  - Random walks: 695\n",
      "  - Vocabulary: 145 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', '.', 'và']\n",
      "\n",
      "🔄 Document 3 (núi):\n",
      "  - Random walks: 695\n",
      "  - Vocabulary: 139 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'là', '.']\n",
      "\n",
      "🔄 Document 4 (núi):\n",
      "  - Random walks: 740\n",
      "  - Vocabulary: 148 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', '.', 'với']\n",
      "\n",
      "🔄 Document 5 (núi):\n",
      "  - Vocabulary: 139 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'là', '.']\n",
      "\n",
      "🔄 Document 4 (núi):\n",
      "  - Random walks: 740\n",
      "  - Vocabulary: 148 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', '.', 'với']\n",
      "\n",
      "🔄 Document 5 (núi):\n",
      "  - Random walks: 770\n",
      "  - Random walks: 770\n",
      "  - Vocabulary: 154 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'của', '.']\n",
      "\n",
      "🔄 Document 6 (biển):\n",
      "  - Random walks: 915\n",
      "  - Vocabulary: 154 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'của', '.']\n",
      "\n",
      "🔄 Document 6 (biển):\n",
      "  - Random walks: 915\n",
      "  - Vocabulary: 183 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'và', '.']\n",
      "\n",
      "🔄 Document 7 (biển):\n",
      "  - Random walks: 930\n",
      "  - Vocabulary: 183 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'và', '.']\n",
      "\n",
      "🔄 Document 7 (biển):\n",
      "  - Random walks: 930\n",
      "  - Vocabulary: 186 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'và', '.']\n",
      "\n",
      "🔄 Document 8 (biển):\n",
      "  - Random walks: 920\n",
      "  - Vocabulary: 186 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'và', '.']\n",
      "\n",
      "🔄 Document 8 (biển):\n",
      "  - Random walks: 920\n",
      "  - Vocabulary: 184 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'và', '.']\n",
      "\n",
      "🔄 Document 9 (biển):\n",
      "  - Random walks: 1020\n",
      "  - Vocabulary: 184 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'và', '.']\n",
      "\n",
      "🔄 Document 9 (biển):\n",
      "  - Random walks: 1020\n",
      "  - Vocabulary: 204 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'và', '.']\n",
      "\n",
      "🔄 Document 10 (biển):\n",
      "  - Random walks: 1025\n",
      "  - Vocabulary: 204 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'và', '.']\n",
      "\n",
      "🔄 Document 10 (biển):\n",
      "  - Random walks: 1025\n",
      "  - Vocabulary: 205 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'và', '.']\n",
      "\n",
      "📊 Successfully generated embeddings for 10 documents\n",
      "  - Vocabulary: 205 words\n",
      "  - Vector size: 64\n",
      "  - Sample words: [',', 'và', '.']\n",
      "\n",
      "📊 Successfully generated embeddings for 10 documents\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all document graphs\n",
    "document_embeddings = {}\n",
    "\n",
    "print(\"🧠 Generating embeddings for all documents...\\n\")\n",
    "\n",
    "for file_num, graph in valid_graphs.items():\n",
    "    print(f\"🔄 Document {file_num} ({LABELS[file_num]}):\")\n",
    "    \n",
    "    # Generate random walks\n",
    "    walker = RandomWalker(graph)\n",
    "    walks = walker.generate_walks()\n",
    "    \n",
    "    print(f\"  - Random walks: {len(walks)}\")\n",
    "    \n",
    "    if walks:\n",
    "        # Train embeddings\n",
    "        deepwalk = DeepWalkEmbedding()\n",
    "        model = deepwalk.train_on_walks(walks)\n",
    "        \n",
    "        if model is not None:\n",
    "            embeddings = deepwalk.get_embeddings()\n",
    "            document_embeddings[file_num] = embeddings\n",
    "            \n",
    "            print(f\"  - Vocabulary: {len(embeddings)} words\")\n",
    "            print(f\"  - Vector size: {VECTOR_SIZE}\")\n",
    "            \n",
    "            # Sample words\n",
    "            sample_words = list(embeddings.keys())[:3]\n",
    "            print(f\"  - Sample words: {sample_words}\")\n",
    "        else:\n",
    "            print(f\"  - ❌ Failed to train embeddings\")\n",
    "    else:\n",
    "        print(f\"  - ❌ No walks generated\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"📊 Successfully generated embeddings for {len(document_embeddings)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b70524",
   "metadata": {},
   "source": [
    "## 📊 Step 4: Document Vector Representation\n",
    "\n",
    "Chuyển đổi node embeddings thành document vectors bằng mean pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31452d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Creating document vectors using mean pooling...\n",
      "\n",
      "🔄 Process: Graph → Node Embeddings → Document Vector (Mean Pooling)\n",
      "============================================================\n",
      "📄 Document 1 (núi):\n",
      "  - Input: 139 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling → Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.505\n",
      "\n",
      "📄 Document 2 (núi):\n",
      "  - Input: 145 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling → Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.558\n",
      "\n",
      "📄 Document 3 (núi):\n",
      "  - Input: 139 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling → Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.486\n",
      "\n",
      "📄 Document 4 (núi):\n",
      "  - Input: 148 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling → Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.575\n",
      "\n",
      "📄 Document 5 (núi):\n",
      "  - Input: 154 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling → Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.456\n",
      "\n",
      "📄 Document 6 (biển):\n",
      "  - Input: 183 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling → Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.462\n",
      "\n",
      "📄 Document 7 (biển):\n",
      "  - Input: 186 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling → Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.516\n",
      "\n",
      "📄 Document 8 (biển):\n",
      "  - Input: 184 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling → Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.441\n",
      "\n",
      "📄 Document 9 (biển):\n",
      "  - Input: 204 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling → Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.528\n",
      "\n",
      "📄 Document 10 (biển):\n",
      "  - Input: 205 node embeddings (size 64 each)\n",
      "  - Process: Mean pooling → Single document vector\n",
      "  - Output: Document vector shape (64,), norm: 1.482\n",
      "\n",
      "📊 Final training dataset:\n",
      "  - Shape: (10, 64) (documents × features)\n",
      "  - Labels: Counter({'núi': 5, 'biển': 5})\n",
      "  - Files: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def create_document_vector(embeddings_dict, method='mean'):\n",
    "    \"\"\"\n",
    "    🎯 CORE FUNCTION: Tạo vector đại diện cho document từ node embeddings\n",
    "    \n",
    "    INPUT: embeddings_dict = {word1: vector1, word2: vector2, ...}\n",
    "    OUTPUT: single vector đại diện cho toàn bộ document\n",
    "    \n",
    "    METHOD: Mean pooling - lấy trung bình của tất cả node embeddings\n",
    "    \"\"\"\n",
    "    if not embeddings_dict:\n",
    "        return np.zeros(VECTOR_SIZE)\n",
    "    \n",
    "    vectors = list(embeddings_dict.values())\n",
    "    \n",
    "    if method == 'mean':\n",
    "        # Mean pooling: vector trung bình của tất cả nodes\n",
    "        return np.mean(vectors, axis=0)\n",
    "    elif method == 'sum':\n",
    "        return np.sum(vectors, axis=0)\n",
    "    elif method == 'max':\n",
    "        return np.max(vectors, axis=0)\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "# ========================================================================\n",
    "# 🎯 BƯỚC QUAN TRỌNG: Tạo document vectors từ graph embeddings\n",
    "# ========================================================================\n",
    "print(\"📊 Creating document vectors using mean pooling...\\n\")\n",
    "print(\"🔄 Process: Graph → Node Embeddings → Document Vector (Mean Pooling)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "document_vectors = {}\n",
    "X = []  # Training features\n",
    "y = []  # Training labels  \n",
    "file_nums = []\n",
    "\n",
    "for file_num in sorted(document_embeddings.keys()):\n",
    "    embeddings = document_embeddings[file_num]\n",
    "    \n",
    "    # 🎯 CORE: Chuyển từ node embeddings thành 1 vector cho document\n",
    "    doc_vector = create_document_vector(embeddings, method='mean')\n",
    "    document_vectors[file_num] = doc_vector\n",
    "    \n",
    "    # Lưu vào training set\n",
    "    X.append(doc_vector)\n",
    "    y.append(LABELS[file_num])\n",
    "    file_nums.append(file_num)\n",
    "    \n",
    "    print(f\"📄 Document {file_num} ({LABELS[file_num]}):\")\n",
    "    print(f\"  - Input: {len(embeddings)} node embeddings (size {VECTOR_SIZE} each)\")\n",
    "    print(f\"  - Process: Mean pooling → Single document vector\")\n",
    "    print(f\"  - Output: Document vector shape {doc_vector.shape}, norm: {np.linalg.norm(doc_vector):.3f}\")\n",
    "    print()\n",
    "\n",
    "# Convert to numpy arrays for machine learning\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"📊 Final training dataset:\")\n",
    "print(f\"  - Shape: {X.shape} (documents × features)\")\n",
    "print(f\"  - Labels: {Counter(y)}\")\n",
    "print(f\"  - Files: {file_nums}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3814b4a1",
   "metadata": {},
   "source": [
    "## 🎯 Step 5: Classification with KNN\n",
    "\n",
    "Huấn luyện mô hình KNN và đánh giá hiệu suất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN classifier on all 10 training documents\n",
    "print(\"🎯 Training KNN classifier on full dataset...\\n\")\n",
    "\n",
    "# Initialize classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=KNN_K, metric='cosine')\n",
    "\n",
    "# Train on full dataset\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Get training set predictions (for reference)\n",
    "y_pred_train = knn.predict(X)\n",
    "y_proba_train = knn.predict_proba(X)\n",
    "\n",
    "print(f\"📊 Training dataset overview:\")\n",
    "print(f\"  - Total samples: {len(X)}\")\n",
    "print(f\"  - Features: {X.shape[1]}\")\n",
    "print(f\"  - Classes: {list(knn.classes_)}\")\n",
    "print(f\"  - KNN k: {KNN_K}\")\n",
    "\n",
    "print(f\"\\n📋 Training set predictions (for reference):\")\n",
    "for i, (file_num, true_label, pred_label) in enumerate(zip(file_nums, y, y_pred_train)):\n",
    "    status = \"✅\" if true_label == pred_label else \"❌\"\n",
    "    confidence = max(y_proba_train[i])\n",
    "    print(f\"  {status} File {file_num}: {true_label} → {pred_label} (conf: {confidence:.3f})\")\n",
    "\n",
    "train_accuracy = accuracy_score(y, y_pred_train)\n",
    "print(f\"\\n🎯 Training accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"📝 Note: High training accuracy is expected (overfitting on small dataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aaf9fc",
   "metadata": {},
   "source": [
    "## 🔮 Step 6: Test Prediction\n",
    "\n",
    "Dự đoán nhãn cho file test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 🔮 BƯỚC DỰ ĐOÁN: Áp dụng pipeline cho test.txt\n",
    "# ========================================================================\n",
    "print(\"🔮 PREDICTION PIPELINE FOR TEST.TXT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"🔄 Process: test.txt → Graph → Embeddings → Vector → KNN Prediction\")\n",
    "print()\n",
    "\n",
    "if test_document:\n",
    "    print(\"🔮 Processing test document...\")\n",
    "    \n",
    "    # STEP 1: Text → Graph\n",
    "    print(\"\\n📊 STEP 1: Building graph for test document\")\n",
    "    test_graph = graph_builder.text_to_graph(test_document)\n",
    "    \n",
    "    print(f\"  - Nodes: {test_graph.number_of_nodes()}\")\n",
    "    print(f\"  - Edges: {test_graph.number_of_edges()}\")\n",
    "    \n",
    "    if test_graph.number_of_nodes() > 0:\n",
    "        # Get largest connected component\n",
    "        if nx.is_connected(test_graph):\n",
    "            test_graph_main = test_graph\n",
    "        else:\n",
    "            test_graph_main = test_graph.subgraph(max(nx.connected_components(test_graph), key=len)).copy()\n",
    "        \n",
    "        print(f\"  - Main component: {test_graph_main.number_of_nodes()} nodes\")\n",
    "        \n",
    "        # STEP 2: Graph → Embeddings  \n",
    "        print(\"\\n🚶 STEP 2: Generating embeddings for test graph\")\n",
    "        test_walker = RandomWalker(test_graph_main)\n",
    "        test_walks = test_walker.generate_walks()\n",
    "        \n",
    "        print(f\"  - Random walks: {len(test_walks)}\")\n",
    "        \n",
    "        if test_walks:\n",
    "            test_deepwalk = DeepWalkEmbedding()\n",
    "            test_model = test_deepwalk.train_on_walks(test_walks)\n",
    "            \n",
    "            if test_model is not None:\n",
    "                test_embeddings = test_deepwalk.get_embeddings()\n",
    "                \n",
    "                print(f\"  - Vocabulary: {len(test_embeddings)} words\")\n",
    "                \n",
    "                # STEP 3: Embeddings → Document Vector\n",
    "                print(\"\\n📊 STEP 3: Creating document vector (mean pooling)\")\n",
    "                test_vector = create_document_vector(test_embeddings, method='mean')\n",
    "                \n",
    "                print(f\"  - Input: {len(test_embeddings)} node embeddings\")\n",
    "                print(f\"  - Output: Document vector norm: {np.linalg.norm(test_vector):.3f}\")\n",
    "                \n",
    "                # STEP 4: Vector → Prediction\n",
    "                print(\"\\n🎯 STEP 4: KNN PREDICTION\")\n",
    "                prediction = knn.predict([test_vector])[0]\n",
    "                probabilities = knn.predict_proba([test_vector])[0]\n",
    "                \n",
    "                print(\"=\" * 40)\n",
    "                print(\"🎯 FINAL PREDICTION RESULTS:\")\n",
    "                print(\"=\" * 40)\n",
    "                print(f\"📄 Test document: {TEST_FILE}\")\n",
    "                print(f\"🔮 Predicted class: **{prediction}**\")\n",
    "                print(f\"📊 Confidence scores:\")\n",
    "                \n",
    "                class_names = knn.classes_\n",
    "                for class_name, prob in zip(class_names, probabilities):\n",
    "                    confidence_bar = \"█\" * int(prob * 20)\n",
    "                    print(f\"     - {class_name}: {prob:.3f} {confidence_bar}\")\n",
    "                \n",
    "                # Nearest neighbors analysis\n",
    "                distances, indices = knn.kneighbors([test_vector])\n",
    "                print(f\"\\n🔍 Nearest neighbors (why this prediction?):\")\n",
    "                for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "                    neighbor_file = file_nums[idx]\n",
    "                    neighbor_label = y[idx]\n",
    "                    print(f\"     {i+1}. File {neighbor_file} ({neighbor_label}): distance = {dist:.3f}\")\n",
    "                \n",
    "                print(\"=\" * 40)\n",
    "                \n",
    "            else:\n",
    "                print(\"  ❌ Failed to train embeddings for test document\")\n",
    "        else:\n",
    "            print(\"  ❌ No walks generated for test document\")\n",
    "    else:\n",
    "        print(\"  ❌ Empty graph for test document\")\n",
    "else:\n",
    "    print(\"❌ No test document available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d8ae8",
   "metadata": {},
   "source": [
    "## 📊 Step 7: Visualization\n",
    "\n",
    "Trực quan hóa 11 document vectors (10 training + 1 test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b37e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 📊 VISUALIZATION: 11 Document Vectors (10 training + 1 test)\n",
    "# ========================================================================\n",
    "print(\"🎨 CREATING COMPREHENSIVE VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"📊 Showing all 11 documents: 10 training + 1 test\")\n",
    "print()\n",
    "\n",
    "if len(X) >= 4:  # Need minimum samples for t-SNE\n",
    "    print(\"🎨 Creating t-SNE visualization for all 11 documents...\")\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    X_viz = X.copy()\n",
    "    y_viz = y.copy()\n",
    "    files_viz = file_nums.copy()\n",
    "    \n",
    "    # Add test document if available\n",
    "    if 'test_vector' in locals():\n",
    "        X_viz = np.vstack([X_viz, test_vector])\n",
    "        y_viz = np.append(y_viz, f\"test_{prediction}\")  # Show predicted class\n",
    "        files_viz.append(\"TEST\")\n",
    "        print(f\"✅ Added test document with predicted label: {prediction}\")\n",
    "    \n",
    "    print(f\"📊 Total documents for visualization: {len(X_viz)}\")\n",
    "    print(f\"   - Training: {len(X)} documents\")\n",
    "    print(f\"   - Test: 1 document\")\n",
    "    print()\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    perplexity = min(5, len(X_viz) - 1)\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=RANDOM_SEED, max_iter=1000)\n",
    "    X_2d = tsne.fit_transform(X_viz)\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot 1: All documents with labels\n",
    "    unique_labels = list(set(y_viz))\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "    color_map = dict(zip(unique_labels, colors))\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        mask = y_viz == label\n",
    "        marker = 'o' if 'test' not in label else '*'  # Star for test document\n",
    "        size = 100 if 'test' not in label else 300  # Larger for test\n",
    "        ax1.scatter(X_2d[mask, 0], X_2d[mask, 1], \n",
    "                   c=[color_map[label]], label=label, s=size, alpha=0.7, marker=marker,\n",
    "                   edgecolors='black' if 'test' in label else 'none', linewidths=2)\n",
    "    \n",
    "    # Add file labels\n",
    "    for i, file_id in enumerate(files_viz):\n",
    "        ax1.annotate(str(file_id), (X_2d[i, 0], X_2d[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points', \n",
    "                    fontsize=10, alpha=0.8, fontweight='bold' if file_id == 'TEST' else 'normal')\n",
    "    \n",
    "    ax1.set_title('📊 All 11 Documents: Training + Test (t-SNE)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('t-SNE Component 1')\n",
    "    ax1.set_ylabel('t-SNE Component 2')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Focus on classification with nearest neighbors\n",
    "    training_mask = y_viz != f\"test_{prediction}\"\n",
    "    test_mask = y_viz == f\"test_{prediction}\"\n",
    "    \n",
    "    # Plot training points by true label\n",
    "    for label in ['núi', 'biển']:\n",
    "        label_mask = y_viz == label\n",
    "        ax2.scatter(X_2d[label_mask, 0], X_2d[label_mask, 1], \n",
    "                   c=[color_map[label]], s=100, alpha=0.7, label=f'Training: {label}')\n",
    "    \n",
    "    # Plot test point\n",
    "    if np.any(test_mask):\n",
    "        ax2.scatter(X_2d[test_mask, 0], X_2d[test_mask, 1], \n",
    "                   c=[color_map[y_viz[test_mask][0]]], s=300, marker='*', \n",
    "                   edgecolors='black', linewidths=2, label=f'Test → {prediction}', zorder=10)\n",
    "        \n",
    "        # Draw lines to nearest neighbors\n",
    "        test_idx = np.where(test_mask)[0][0]\n",
    "        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "            ax2.plot([X_2d[test_idx, 0], X_2d[idx, 0]], \n",
    "                    [X_2d[test_idx, 1], X_2d[idx, 1]], \n",
    "                    'k--', alpha=0.3, linewidth=1)\n",
    "            # Add distance annotation\n",
    "            mid_x = (X_2d[test_idx, 0] + X_2d[idx, 0]) / 2\n",
    "            mid_y = (X_2d[test_idx, 1] + X_2d[idx, 1]) / 2\n",
    "            ax2.annotate(f'd={dist:.2f}', (mid_x, mid_y), \n",
    "                        fontsize=8, alpha=0.6, ha='center')\n",
    "    \n",
    "    # Add file labels\n",
    "    for i, file_id in enumerate(files_viz):\n",
    "        color = 'red' if file_id == 'TEST' else 'black'\n",
    "        weight = 'bold' if file_id == 'TEST' else 'normal'\n",
    "        ax2.annotate(str(file_id), (X_2d[i, 0], X_2d[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points', \n",
    "                    fontsize=10, alpha=0.8, color=color, fontweight=weight)\n",
    "    \n",
    "    ax2.set_title('🎯 KNN Classification: Test with Nearest Neighbors', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('t-SNE Component 1')\n",
    "    ax2.set_ylabel('t-SNE Component 2')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n📊 VISUALIZATION SUMMARY:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"📈 Total points visualized: {len(X_viz)}\")\n",
    "    print(f\"🎯 Test document: {TEST_FILE}\")\n",
    "    print(f\"🔮 Predicted as: {prediction}\")\n",
    "    print(f\"📍 Test point marked with ⭐ (star)\")\n",
    "    print(f\"🔗 Dashed lines show k={KNN_K} nearest neighbors\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Insufficient data for t-SNE visualization (need >= 4 samples)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
