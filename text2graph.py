import streamlit as st
import pandas as pd
import networkx as nx
from pyvis.network import Network
from underthesea import word_tokenize
import os
import numpy as np
from collections import Counter, defaultdict
from itertools import combinations
import math

# Cáº¥u hÃ¬nh trang
st.set_page_config(
    page_title="Text to Graph Visualization",
    page_icon="ğŸ•¸ï¸",
    layout="wide"
)

class TextGraphBuilder:
    def __init__(self, window_size=3, weight_method='frequency'):
        self.window_size = window_size
        self.weight_method = weight_method
        self.vocab = set()
        self.cooccurrence = defaultdict(int)
        self.word_counts = Counter()
        self.total_words = 0
        
    def process_text(self, text):
        """Tokenize vÃ  xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t"""
        # Tokenize báº±ng underthesea - giá»¯ nguyÃªn tá»« ghÃ©p vá»›i dáº¥u gáº¡ch dÆ°á»›i
        tokens = word_tokenize(text.lower())
        
        # Lá»c bá» dáº¥u cÃ¢u vÃ  tá»« quÃ¡ ngáº¯n, giá»¯ nguyÃªn dáº¥u gáº¡ch dÆ°á»›i cho tá»« ghÃ©p
        filtered_tokens = []
        for token in tokens:
            # Loáº¡i bá» khoáº£ng tráº¯ng Ä‘áº§u cuá»‘i
            token = token.strip()
            # Chá»‰ giá»¯ tá»« cÃ³ Ä‘á»™ dÃ i > 1 (bá» isalnum Ä‘á»ƒ giá»¯ láº¡i tá»« ghÃ©p cÃ³ dáº¥u '_')
            if len(token) > 1:
                filtered_tokens.append(token)
        
        return filtered_tokens
    
    def build_cooccurrence_matrix(self, tokens):
        """XÃ¢y dá»±ng ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n"""
        self.vocab.update(tokens)
        self.word_counts.update(tokens)
        self.total_words += len(tokens)
        
        # Sliding window Ä‘á»ƒ tÃ­nh Ä‘á»“ng xuáº¥t hiá»‡n
        for i in range(len(tokens)):
            for j in range(max(0, i - self.window_size), 
                          min(len(tokens), i + self.window_size + 1)):
                if i != j:
                    word1, word2 = tokens[i], tokens[j]
                    # Sáº¯p xáº¿p Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n
                    if word1 != word2:
                        pair = tuple(sorted([word1, word2]))
                        self.cooccurrence[pair] += 1
    
    def calculate_pmi(self, word1, word2, cooc_count):
        """TÃ­nh Pointwise Mutual Information (PMI)"""
        if self.total_words == 0:
            return 0
            
        # P(word1, word2)
        p_joint = cooc_count / self.total_words
        
        # P(word1) vÃ  P(word2)
        p_word1 = self.word_counts[word1] / self.total_words
        p_word2 = self.word_counts[word2] / self.total_words
        
        if p_word1 == 0 or p_word2 == 0 or p_joint == 0:
            return 0
            
        # PMI = log(P(word1, word2) / (P(word1) * P(word2)))
        pmi = math.log2(p_joint / (p_word1 * p_word2))
        return max(0, pmi)  # Positive PMI
    
    def build_graph(self, min_frequency=1):
        """XÃ¢y dá»±ng Ä‘á»“ thá»‹ tá»« ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n"""
        G = nx.Graph()
        
        # Lá»c tá»« theo táº§n suáº¥t (bá» giá»›i háº¡n sá»‘ lÆ°á»£ng nÃºt)
        frequent_words = [word for word, count in self.word_counts.most_common() 
                         if count >= min_frequency]
        
        # ThÃªm nÃºt
        for word in frequent_words:
            G.add_node(word, frequency=self.word_counts[word])
        
        # ThÃªm cáº¡nh
        edges_added = 0
        for (word1, word2), cooc_count in self.cooccurrence.items():
            if word1 in frequent_words and word2 in frequent_words and cooc_count >= min_frequency:
                if self.weight_method == 'frequency':
                    weight = cooc_count
                else:  # PMI
                    weight = self.calculate_pmi(word1, word2, cooc_count)
                
                if weight > 0:
                    G.add_edge(word1, word2, weight=weight)
                    edges_added += 1
        
        return G

def load_data_files():
    """Load táº¥t cáº£ cÃ¡c file txt tá»« thÆ° má»¥c data"""
    data_folder = "data"
    files = []
    texts = {}
    
    if os.path.exists(data_folder):
        for filename in os.listdir(data_folder):
            if filename.endswith('.txt'):
                filepath = os.path.join(data_folder, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                        texts[filename] = content
                        files.append(filename)
                except Exception as e:
                    st.error(f"KhÃ´ng thá»ƒ Ä‘á»c file {filename}: {e}")
    
    return files, texts

def visualize_graph(G):
    """Trá»±c quan hÃ³a Ä‘á»“ thá»‹ báº±ng Pyvis vÃ  nhÃºng vÃ o Streamlit vá»›i layout Spring"""
    if len(G.nodes()) == 0:
        st.warning("Äá»“ thá»‹ khÃ´ng cÃ³ nÃºt nÃ o. HÃ£y thá»­ giáº£m ngÆ°á»¡ng táº§n suáº¥t tá»‘i thiá»ƒu.")
        return None
    
    net = Network(height="600px", width="100%", notebook=False, directed=False)
    net.barnes_hut()
    
    # ThÃªm nÃºt vá»›i kÃ­ch thÆ°á»›c báº±ng nhau
    for node in G.nodes():
        freq = G.nodes[node].get('frequency', 1)
        # Láº¥y danh sÃ¡ch cÃ¡c node ká» cáº­n
        neighbors = list(G.neighbors(node))
        # Hiá»ƒn thá»‹ tá»« ká» cáº­n vá»›i khoáº£ng tráº¯ng thay vÃ¬ underscore
        neighbors_display = [n.replace('_', ' ') for n in neighbors[:10]]
        neighbors_str = ', '.join(neighbors_display)
        if len(neighbors) > 10:
            neighbors_str += f' ... (vÃ  {len(neighbors) - 10} tá»« khÃ¡c)'
        
        # Thay tháº¿ _ báº±ng khoáº£ng tráº¯ng chá»‰ khi hiá»ƒn thá»‹
        display_label = node.replace('_', ' ')
        
        # Táº¡o tooltip vá»›i HTML Ä‘Ãºng format
        tooltip = f"""Tá»«: {display_label}
Táº§n suáº¥t: {freq}
Báº­c: {G.degree[node]}
CÃ¡c tá»« liÃªn káº¿t: {neighbors_str}"""
        
        net.add_node(
            node,
            label=display_label,  # Hiá»ƒn thá»‹ vá»›i khoáº£ng tráº¯ng
            title=tooltip,
            size=20,  # KÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh cho táº¥t cáº£ cÃ¡c nÃºt
        )
    
    # ThÃªm cáº¡nh vá»›i Ä‘á»™ dÃ y cá»‘ Ä‘á»‹nh vÃ  mÃ u xÃ¡m
    for edge in G.edges():
        w = G[edge[0]][edge[1]].get('weight', 1)
        net.add_edge(edge[0], edge[1], width=1, color='#808080', title=f"Trá»ng sá»‘: {w:.2f}")
    
    # TÃ¹y chá»‰nh options: sá»­ dá»¥ng Spring layout vÃ  táº¯t hiá»‡u á»©ng xoay vÃ²ng
    net.set_options('''
    var options = {
        "nodes": {
            "font": {"size": 18},
            "size": 20,
            "borderWidth": 2
        },
        "edges": {
            "color": {"color": "#808080"},
            "width": 1,
            "smooth": false
        },
        "layout": {
            "improvedLayout": false
        },
        "physics": {
            "enabled": true,
            "forceAtlas2Based": {
                "gravitationalConstant": -50,
                "centralGravity": 0.01,
                "springLength": 100,
                "springConstant": 0.08
            },
            "maxVelocity": 146,
            "solver": "forceAtlas2Based",
            "timestep": 0.35,
            "stabilization": {
                "enabled": true,
                "iterations": 150,
                "updateInterval": 25,
                "fit": true
            }
        },
        "interaction": {
            "dragNodes": true,
            "dragView": true,
            "zoomView": true
        }
    }
    ''')
    
    # LÆ°u HTML táº¡m thá»i vÃ  nhÃºng vÃ o Streamlit
    import tempfile
    with tempfile.NamedTemporaryFile('w', delete=False, suffix='.html') as f:
        net.save_graph(f.name)
        html_content = open(f.name, 'r', encoding='utf-8').read()
    
    # Sá»­ dá»¥ng width Ä‘á»ƒ trÃ¡nh lá»—i vá»›i st.components.v1.html
    st.components.v1.html(html_content, height=650, scrolling=True)
    return None

def main():
    st.title("ğŸ•¸ï¸ Text to Graph Visualization")
    st.markdown("**Trá»±c quan hÃ³a quÃ¡ trÃ¬nh xÃ¢y dá»±ng Ä‘á»“ thá»‹ tá»« vÄƒn báº£n tiáº¿ng Viá»‡t**")
    
    # Sidebar Ä‘á»ƒ chá»‰nh tham sá»‘
    st.sidebar.header("âš™ï¸ Tham sá»‘")
    
    # Load dá»¯ liá»‡u
    files, texts = load_data_files()
    
    if not files:
        st.error("KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u trong thÆ° má»¥c 'data'")
        return
    
    # Chá»n file
    selected_files = st.sidebar.multiselect(
        "Chá»n file Ä‘á»ƒ phÃ¢n tÃ­ch:",
        files,
        default=files[:3] if len(files) >= 3 else files
    )
    
    # Tham sá»‘ sliding window
    window_size = st.sidebar.slider(
        "KÃ­ch thÆ°á»›c cá»­a sá»• ngá»¯ cáº£nh (k):",
        min_value=1,
        max_value=10,
        value=3,
        help="Sá»‘ tá»« xung quanh Ä‘á»ƒ tÃ­nh Ä‘á»“ng xuáº¥t hiá»‡n"
    )
    
    # PhÆ°Æ¡ng phÃ¡p tÃ­nh trá»ng sá»‘
    weight_method = st.sidebar.selectbox(
        "PhÆ°Æ¡ng phÃ¡p tÃ­nh trá»ng sá»‘ cáº¡nh:",
        ["frequency", "pmi"],
        format_func=lambda x: "Táº§n suáº¥t" if x == "frequency" else "PMI (Pointwise Mutual Information)"
    )
    
    # Tham sá»‘ lá»c
    min_frequency = st.sidebar.slider(
        "Táº§n suáº¥t tá»‘i thiá»ƒu:",
        min_value=1,
        max_value=10,
        value=2,
        help="Tá»« pháº£i xuáº¥t hiá»‡n Ã­t nháº¥t bao nhiÃªu láº§n"
    )
    
    if not selected_files:
        st.warning("Vui lÃ²ng chá»n Ã­t nháº¥t má»™t file Ä‘á»ƒ phÃ¢n tÃ­ch.")
        return
    
    # Xá»­ lÃ½ dá»¯ liá»‡u
    with st.spinner("Äang xá»­ lÃ½ vÄƒn báº£n..."):
        # Khá»Ÿi táº¡o TextGraphBuilder
        graph_builder = TextGraphBuilder(window_size=window_size, weight_method=weight_method)
        # Xá»­ lÃ½ tá»«ng file Ä‘Æ°á»£c chá»n
        all_text = ""
        for filename in selected_files:
            all_text += texts[filename] + " "
        # Tokenize toÃ n bá»™ vÄƒn báº£n
        tokens = graph_builder.process_text(all_text)
        # XÃ¢y dá»±ng ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n
        graph_builder.build_cooccurrence_matrix(tokens)
        # XÃ¢y dá»±ng Ä‘á»“ thá»‹ (khÃ´ng giá»›i háº¡n sá»‘ nÃºt)
        G = graph_builder.build_graph(min_frequency=min_frequency)
    
    # Hiá»ƒn thá»‹ thá»‘ng kÃª
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Sá»‘ file Ä‘Ã£ chá»n", len(selected_files))
    
    with col2:
        st.metric("Tá»•ng sá»‘ tá»«", len(tokens), help="Tá»•ng sá»‘ tá»« trong vÄƒn báº£n (ká»ƒ cáº£ tá»« láº·p láº¡i)")
    
    with col3:
        st.metric("Tá»« vá»±ng duy nháº¥t", len(graph_builder.vocab), help="Sá»‘ lÆ°á»£ng tá»« khÃ¡c nhau (khÃ´ng Ä‘áº¿m trÃ¹ng láº·p)")
    
    with col4:
        st.metric("Cáº·p tá»« Ä‘á»“ng xuáº¥t hiá»‡n", len(graph_builder.cooccurrence), help="Sá»‘ cáº·p tá»« xuáº¥t hiá»‡n gáº§n nhau trong ngá»¯ cáº£nh")
    
    # Hiá»ƒn thá»‹ Ä‘á»“ thá»‹
    st.header("ğŸ“Š Äá»“ thá»‹ tá»« vá»±ng")
    if len(G.nodes()) > 0:
        visualize_graph(G)
        # Hiá»ƒn thá»‹ thÃ´ng tin Ä‘á»“ thá»‹
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ¯ ThÃ´ng tin Ä‘á»“ thá»‹")
            st.write(f"**Sá»‘ nÃºt:** {len(G.nodes())}")
            st.write(f"**Sá»‘ cáº¡nh:** {len(G.edges())}")
            st.write(f"**Máº­t Ä‘á»™:** {nx.density(G):.4f}")
            if len(G.nodes()) > 0:
                st.write(f"**Báº­c trung bÃ¬nh:** {sum(dict(G.degree()).values()) / len(G.nodes()):.2f}")
        with col2:
            st.subheader("ğŸ”¥ Top tá»« cÃ³ táº§n suáº¥t cao")
            top_words = graph_builder.word_counts.most_common(10)
            df_top = pd.DataFrame(top_words, columns=['Tá»«', 'Táº§n suáº¥t'])
            st.dataframe(df_top, width='stretch')
    else:
        st.warning("KhÃ´ng thá»ƒ táº¡o Ä‘á»“ thá»‹ vá»›i cÃ¡c tham sá»‘ hiá»‡n táº¡i. HÃ£y thá»­ giáº£m ngÆ°á»¡ng táº§n suáº¥t tá»‘i thiá»ƒu.")
    
    # Hiá»ƒn thá»‹ má»™t sá»‘ cáº·p tá»« Ä‘á»“ng xuáº¥t hiá»‡n
    if graph_builder.cooccurrence:
        st.subheader("ğŸ”— Top cáº·p tá»« Ä‘á»“ng xuáº¥t hiá»‡n")
        top_cooc = sorted(graph_builder.cooccurrence.items(), key=lambda x: x[1], reverse=True)[:15]
        
        cooc_data = []
        for (word1, word2), count in top_cooc:
            if weight_method == 'pmi':
                pmi_score = graph_builder.calculate_pmi(word1, word2, count)
                cooc_data.append([f"{word1} - {word2}", count, f"{pmi_score:.3f}"])
            else:
                cooc_data.append([f"{word1} - {word2}", count, count])
        
        columns = ['Cáº·p tá»«', 'Táº§n suáº¥t', 'PMI' if weight_method == 'pmi' else 'Trá»ng sá»‘']
        df_cooc = pd.DataFrame(cooc_data, columns=columns)
        st.dataframe(df_cooc, width='stretch')
    
    # Hiá»ƒn thá»‹ ná»™i dung file Ä‘Æ°á»£c chá»n
    with st.expander("ğŸ“„ Xem ná»™i dung file Ä‘Ã£ chá»n"):
        for filename in selected_files:
            st.subheader(f"File: {filename}")
            st.text_area("Ná»™i dung:", texts[filename], height=150, key=filename)

if __name__ == "__main__":
    main()
