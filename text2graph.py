import streamlit as st
import pandas as pd
import networkx as nx
from pyvis.network import Network
from underthesea import word_tokenize
import os
import numpy as np
from collections import Counter, defaultdict
from itertools import combinations
import math

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="Text to Graph Visualization",
    page_icon="üï∏Ô∏è",
    layout="wide"
)

class TextGraphBuilder:
    def __init__(self, window_size=3, weight_method='frequency'):
        self.window_size = window_size
        self.weight_method = weight_method
        self.vocab = set()
        self.cooccurrence = defaultdict(int)
        self.word_counts = Counter()
        self.total_words = 0
        
    def process_text(self, text):
        """Tokenize v√† x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát"""
        # Tokenize b·∫±ng underthesea - gi·ªØ nguy√™n t·ª´ gh√©p v·ªõi d·∫•u g·∫°ch d∆∞·ªõi
        tokens = word_tokenize(text.lower())
        
        # L·ªçc b·ªè d·∫•u c√¢u v√† t·ª´ qu√° ng·∫Øn, gi·ªØ nguy√™n d·∫•u g·∫°ch d∆∞·ªõi cho t·ª´ gh√©p
        filtered_tokens = []
        for token in tokens:
            # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng ƒë·∫ßu cu·ªëi
            token = token.strip()
            # Ch·ªâ gi·ªØ t·ª´ c√≥ ƒë·ªô d√†i > 1 (b·ªè isalnum ƒë·ªÉ gi·ªØ l·∫°i t·ª´ gh√©p c√≥ d·∫•u '_')
            if len(token) > 1:
                filtered_tokens.append(token)
        
        return filtered_tokens
    
    def build_cooccurrence_matrix(self, tokens):
        """X√¢y d·ª±ng ma tr·∫≠n ƒë·ªìng xu·∫•t hi·ªán"""
        self.vocab.update(tokens)
        self.word_counts.update(tokens)
        self.total_words += len(tokens)
        
        # Sliding window ƒë·ªÉ t√≠nh ƒë·ªìng xu·∫•t hi·ªán
        for i in range(len(tokens)):
            for j in range(max(0, i - self.window_size), 
                          min(len(tokens), i + self.window_size + 1)):
                if i != j:
                    word1, word2 = tokens[i], tokens[j]
                    # S·∫Øp x·∫øp ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n
                    if word1 != word2:
                        pair = tuple(sorted([word1, word2]))
                        self.cooccurrence[pair] += 1
    
    def calculate_pmi(self, word1, word2, cooc_count):
        """T√≠nh Pointwise Mutual Information (PMI)"""
        if self.total_words == 0:
            return 0
            
        # P(word1, word2)
        p_joint = cooc_count / self.total_words
        
        # P(word1) v√† P(word2)
        p_word1 = self.word_counts[word1] / self.total_words
        p_word2 = self.word_counts[word2] / self.total_words
        
        if p_word1 == 0 or p_word2 == 0 or p_joint == 0:
            return 0
            
        # PMI = log(P(word1, word2) / (P(word1) * P(word2)))
        pmi = math.log2(p_joint / (p_word1 * p_word2))
        return max(0, pmi)  # Positive PMI
    
    def build_graph(self, min_frequency=1):
        """X√¢y d·ª±ng ƒë·ªì th·ªã t·ª´ ma tr·∫≠n ƒë·ªìng xu·∫•t hi·ªán"""
        G = nx.Graph()
        
        # L·ªçc t·ª´ theo t·∫ßn su·∫•t (b·ªè gi·ªõi h·∫°n s·ªë l∆∞·ª£ng n√∫t)
        frequent_words = [word for word, count in self.word_counts.most_common() 
                         if count >= min_frequency]
        
        # Th√™m n√∫t
        for word in frequent_words:
            G.add_node(word, frequency=self.word_counts[word])
        
        # Th√™m c·∫°nh
        edges_added = 0
        for (word1, word2), cooc_count in self.cooccurrence.items():
            if word1 in frequent_words and word2 in frequent_words and cooc_count >= min_frequency:
                if self.weight_method == 'frequency':
                    weight = cooc_count
                else:  # PMI
                    weight = self.calculate_pmi(word1, word2, cooc_count)
                
                if weight > 0:
                    G.add_edge(word1, word2, weight=weight)
                    edges_added += 1
        
        return G

def load_data_files():
    """Load t·∫•t c·∫£ c√°c file txt t·ª´ th∆∞ m·ª•c data"""
    data_folder = "data"
    files = []
    texts = {}
    
    if os.path.exists(data_folder):
        for filename in os.listdir(data_folder):
            if filename.endswith('.txt'):
                filepath = os.path.join(data_folder, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                        texts[filename] = content
                        files.append(filename)
                except Exception as e:
                    st.error(f"Kh√¥ng th·ªÉ ƒë·ªçc file {filename}: {e}")
    
    return files, texts

def visualize_graph(G):
    """Tr·ª±c quan h√≥a ƒë·ªì th·ªã b·∫±ng Pyvis v√† nh√∫ng v√†o Streamlit v·ªõi layout Spring"""
    if len(G.nodes()) == 0:
        st.warning("ƒê·ªì th·ªã kh√¥ng c√≥ n√∫t n√†o. H√£y th·ª≠ gi·∫£m ng∆∞·ª°ng t·∫ßn su·∫•t t·ªëi thi·ªÉu.")
        return None
    
    net = Network(height="600px", width="100%", notebook=False, directed=False)
    net.barnes_hut()
    
    # Th√™m n√∫t v·ªõi k√≠ch th∆∞·ªõc b·∫±ng nhau
    for node in G.nodes():
        freq = G.nodes[node].get('frequency', 1)
        # L·∫•y danh s√°ch c√°c node k·ªÅ c·∫≠n
        neighbors = list(G.neighbors(node))
        # Hi·ªÉn th·ªã t·ª´ k·ªÅ c·∫≠n v·ªõi kho·∫£ng tr·∫Øng thay v√¨ underscore
        neighbors_display = [n.replace('_', ' ') for n in neighbors[:10]]
        neighbors_str = ', '.join(neighbors_display)
        if len(neighbors) > 10:
            neighbors_str += f' ... (v√† {len(neighbors) - 10} t·ª´ kh√°c)'
        
        # Thay th·∫ø _ b·∫±ng kho·∫£ng tr·∫Øng ch·ªâ khi hi·ªÉn th·ªã
        display_label = node.replace('_', ' ')
        
        # T·∫°o tooltip v·ªõi HTML ƒë√∫ng format
        tooltip = f"""T·ª´: {display_label}
T·∫ßn su·∫•t: {freq}
B·∫≠c: {G.degree[node]}
C√°c t·ª´ li√™n k·∫øt: {neighbors_str}"""
        
        net.add_node(
            node,
            label=display_label,  # Hi·ªÉn th·ªã v·ªõi kho·∫£ng tr·∫Øng
            title=tooltip,
            size=20,  # K√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh cho t·∫•t c·∫£ c√°c n√∫t
        )
    
    # Th√™m c·∫°nh v·ªõi ƒë·ªô d√†y c·ªë ƒë·ªãnh v√† m√†u x√°m
    for edge in G.edges():
        w = G[edge[0]][edge[1]].get('weight', 1)
        net.add_edge(edge[0], edge[1], width=1, color='#808080', title=f"Tr·ªçng s·ªë: {w:.2f}")
    
    # T√πy ch·ªânh options: s·ª≠ d·ª•ng Spring layout v√† t·∫Øt hi·ªáu ·ª©ng xoay v√≤ng
    net.set_options('''
    var options = {
        "nodes": {
            "font": {"size": 18},
            "size": 20,
            "borderWidth": 2
        },
        "edges": {
            "color": {"color": "#808080"},
            "width": 1,
            "smooth": false
        },
        "layout": {
            "improvedLayout": false
        },
        "physics": {
            "enabled": true,
            "forceAtlas2Based": {
                "gravitationalConstant": -50,
                "centralGravity": 0.01,
                "springLength": 100,
                "springConstant": 0.08
            },
            "maxVelocity": 146,
            "solver": "forceAtlas2Based",
            "timestep": 0.35,
            "stabilization": {
                "enabled": true,
                "iterations": 150,
                "updateInterval": 25,
                "fit": true
            }
        },
        "interaction": {
            "dragNodes": true,
            "dragView": true,
            "zoomView": true
        }
    }
    ''')
    
    # L∆∞u HTML t·∫°m th·ªùi v√† nh√∫ng v√†o Streamlit
    import tempfile
    with tempfile.NamedTemporaryFile('w', delete=False, suffix='.html') as f:
        net.save_graph(f.name)
        html_content = open(f.name, 'r', encoding='utf-8').read()
    
    # S·ª≠ d·ª•ng width ƒë·ªÉ tr√°nh l·ªói v·ªõi st.components.v1.html
    st.components.v1.html(html_content, height=650, scrolling=True)
    return None

def main():
    st.title("üï∏Ô∏è Text to Graph Visualization")
    st.markdown("**Tr·ª±c quan h√≥a qu√° tr√¨nh x√¢y d·ª±ng ƒë·ªì th·ªã t·ª´ vƒÉn b·∫£n ti·∫øng Vi·ªát**")
    
    # Sidebar ƒë·ªÉ ch·ªânh tham s·ªë
    st.sidebar.header("‚öôÔ∏è Tham s·ªë")
    
    # Load d·ªØ li·ªáu
    files, texts = load_data_files()
    
    if not files:
        st.error("Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu trong th∆∞ m·ª•c 'data'")
        return
    
    # Ch·ªçn file
    selected_files = st.sidebar.multiselect(
        "Ch·ªçn file ƒë·ªÉ ph√¢n t√≠ch:",
        files,
        default=files[:3] if len(files) >= 3 else files
    )
    
    # Tham s·ªë sliding window
    window_size = st.sidebar.slider(
        "K√≠ch th∆∞·ªõc c·ª≠a s·ªï ng·ªØ c·∫£nh (k):",
        min_value=1,
        max_value=10,
        value=3,
        help="S·ªë t·ª´ xung quanh ƒë·ªÉ t√≠nh ƒë·ªìng xu·∫•t hi·ªán"
    )
    
    # Ph∆∞∆°ng ph√°p t√≠nh tr·ªçng s·ªë
    weight_method = st.sidebar.selectbox(
        "Ph∆∞∆°ng ph√°p t√≠nh tr·ªçng s·ªë c·∫°nh:",
        ["frequency", "pmi"],
        format_func=lambda x: "T·∫ßn su·∫•t" if x == "frequency" else "PMI (Pointwise Mutual Information)"
    )
    
    # Tham s·ªë l·ªçc
    min_frequency = st.sidebar.slider(
        "T·∫ßn su·∫•t t·ªëi thi·ªÉu:",
        min_value=1,
        max_value=10,
        value=2,
        help="T·ª´ ph·∫£i xu·∫•t hi·ªán √≠t nh·∫•t bao nhi√™u l·∫ßn"
    )
    
    if not selected_files:
        st.warning("Vui l√≤ng ch·ªçn √≠t nh·∫•t m·ªôt file ƒë·ªÉ ph√¢n t√≠ch.")
        return
    
    # X·ª≠ l√Ω d·ªØ li·ªáu
    with st.spinner("ƒêang x·ª≠ l√Ω vƒÉn b·∫£n..."):
        # Kh·ªüi t·∫°o TextGraphBuilder
        graph_builder = TextGraphBuilder(window_size=window_size, weight_method=weight_method)
        # X·ª≠ l√Ω t·ª´ng file ƒë∆∞·ª£c ch·ªçn
        all_text = ""
        for filename in selected_files:
            all_text += texts[filename] + " "
        # Tokenize to√†n b·ªô vƒÉn b·∫£n
        tokens = graph_builder.process_text(all_text)
        # X√¢y d·ª±ng ma tr·∫≠n ƒë·ªìng xu·∫•t hi·ªán
        graph_builder.build_cooccurrence_matrix(tokens)
        # X√¢y d·ª±ng ƒë·ªì th·ªã (kh√¥ng gi·ªõi h·∫°n s·ªë n√∫t)
        G = graph_builder.build_graph(min_frequency=min_frequency)
    
    # Hi·ªÉn th·ªã th·ªëng k√™
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("S·ªë file ƒë√£ ch·ªçn", len(selected_files))
    
    with col2:
        st.metric("T·ªïng s·ªë t·ª´", len(tokens), help="T·ªïng s·ªë t·ª´ trong vƒÉn b·∫£n (k·ªÉ c·∫£ t·ª´ l·∫∑p l·∫°i)")
    
    with col3:
        st.metric("T·ª´ v·ª±ng duy nh·∫•t", len(graph_builder.vocab), help="S·ªë l∆∞·ª£ng t·ª´ kh√°c nhau (kh√¥ng ƒë·∫øm tr√πng l·∫∑p)")
    
    with col4:
        st.metric("C·∫∑p t·ª´ ƒë·ªìng xu·∫•t hi·ªán", len(graph_builder.cooccurrence), help="S·ªë c·∫∑p t·ª´ xu·∫•t hi·ªán g·∫ßn nhau trong ng·ªØ c·∫£nh")
    
    # Hi·ªÉn th·ªã ƒë·ªì th·ªã
    st.header("üìä ƒê·ªì th·ªã t·ª´ v·ª±ng")
    if len(G.nodes()) > 0:
        visualize_graph(G)
        # Hi·ªÉn th·ªã th√¥ng tin ƒë·ªì th·ªã
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("üéØ Th√¥ng tin ƒë·ªì th·ªã")
            st.write(f"**S·ªë n√∫t:** {len(G.nodes())}")
            st.write(f"**S·ªë c·∫°nh:** {len(G.edges())}")
            st.write(f"**M·∫≠t ƒë·ªô:** {nx.density(G):.4f}")
            if len(G.nodes()) > 0:
                st.write(f"**B·∫≠c trung b√¨nh:** {sum(dict(G.degree()).values()) / len(G.nodes()):.2f}")
        with col2:
            st.subheader("üî• Top t·ª´ c√≥ t·∫ßn su·∫•t cao")
            top_words = graph_builder.word_counts.most_common(10)
            df_top = pd.DataFrame(top_words, columns=['T·ª´', 'T·∫ßn su·∫•t'])
            st.dataframe(df_top, width='stretch')
    else:
        st.warning("Kh√¥ng th·ªÉ t·∫°o ƒë·ªì th·ªã v·ªõi c√°c tham s·ªë hi·ªán t·∫°i. H√£y th·ª≠ gi·∫£m ng∆∞·ª°ng t·∫ßn su·∫•t t·ªëi thi·ªÉu.")
    
    # Hi·ªÉn th·ªã m·ªôt s·ªë c·∫∑p t·ª´ ƒë·ªìng xu·∫•t hi·ªán
    if graph_builder.cooccurrence:
        st.subheader("üîó Top c·∫∑p t·ª´ ƒë·ªìng xu·∫•t hi·ªán")
        top_cooc = sorted(graph_builder.cooccurrence.items(), key=lambda x: x[1], reverse=True)[:15]
        
        cooc_data = []
        for (word1, word2), count in top_cooc:
            if weight_method == 'pmi':
                pmi_score = graph_builder.calculate_pmi(word1, word2, count)
                cooc_data.append([f"{word1} - {word2}", count, f"{pmi_score:.3f}"])
            else:
                cooc_data.append([f"{word1} - {word2}", count, count])
        
        columns = ['C·∫∑p t·ª´', 'T·∫ßn su·∫•t', 'PMI' if weight_method == 'pmi' else 'Tr·ªçng s·ªë']
        df_cooc = pd.DataFrame(cooc_data, columns=columns)
        st.dataframe(df_cooc, width='stretch')
    
    # Hi·ªÉn th·ªã n·ªôi dung file ƒë∆∞·ª£c ch·ªçn
    with st.expander("üìÑ Xem n·ªôi dung file ƒë√£ ch·ªçn"):
        for filename in selected_files:
            st.subheader(f"File: {filename}")
            st.text_area("N·ªôi dung:", texts[filename], height=150, key=filename)

if __name__ == "__main__":
    main()
